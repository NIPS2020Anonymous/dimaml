{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "import os, sys, time\n",
    "import warnings\n",
    "sys.path.insert(0, '..')\n",
    "import lib\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# For reproducibility\n",
    "import random\n",
    "seed = random.randint(0, 2 ** 32 - 1)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'fixup_resnet'\n",
    "\n",
    "# Dataset \n",
    "data_dir = './data'\n",
    "train_batch_size = 128\n",
    "valid_batch_size = 128\n",
    "test_batch_size = 64\n",
    "num_workers = 3\n",
    "pin_memory = True\n",
    "\n",
    "num_classes = 100\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "loss_function = F.cross_entropy\n",
    "\n",
    "# MAML\n",
    "max_epochs = 2000\n",
    "maml_n_var_batches = 190\n",
    "\n",
    "maml_steps = 190\n",
    "max_grad_norm = 4.\n",
    "max_meta_parameters_grad_norm = 10.\n",
    "\n",
    "loss_kwargs={'reduction':'mean'}\n",
    "\n",
    "first_valid_step = 38\n",
    "loss_interval = 38\n",
    "\n",
    "assert (maml_steps - first_valid_step) % loss_interval == 0\n",
    "validation_steps = (maml_steps - first_valid_step) / loss_interval + 1\n",
    "\n",
    "# Optimizer\n",
    "optimizer_type='momentum'\n",
    "nesterov = True\n",
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0005\n",
    "\n",
    "# Meta optimizer\n",
    "meta_betas = (0.9, 0.997)\n",
    "meta_learning_rate = 0.0001\n",
    "\n",
    "checkpoint_steps = 3\n",
    "recovery_step = None\n",
    "\n",
    "\n",
    "kwargs = dict(\n",
    "    first_valid_step=first_valid_step,\n",
    "    valid_loss_interval=loss_interval, \n",
    "    loss_kwargs=loss_kwargs, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = f\"FixupResNet18_CIFAR100_{optimizer_type}_lr{learning_rate}\"\n",
    "exp_name += f\"_meta_rl{meta_learning_rate}_steps{maml_steps}_interval{loss_interval}\"\n",
    "exp_name += f\"_var_batches{maml_n_var_batches}_tr_bs{train_batch_size}_val_bs{valid_batch_size}_seed_{seed}\"\n",
    "print(\"Experiment name: \", exp_name)\n",
    "\n",
    "logs_path = \"./logs/{}\".format(exp_name)\n",
    "assert recovery_step is not None or not os.path.exists(logs_path)\n",
    "# !rm -rf {logs_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and valid data\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762)),\n",
    "])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762)),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR100(root=data_dir, train=True, download=True, transform=train_transform)\n",
    "valid_dataset = datasets.CIFAR100(root=data_dir, train=True, download=True, transform=eval_transform)\n",
    "test_set = datasets.CIFAR100(root=data_dir, train=False, download=True, transform=eval_transform)\n",
    "\n",
    "num_train = len(train_dataset)\n",
    "indices = list(range(num_train))\n",
    "split = 40000\n",
    "\n",
    "if maml_n_var_batches * train_batch_size >= split:\n",
    "    warnings.warn(\"Your training process involves one entire epoch\")\n",
    "    \n",
    "np.random.shuffle(indices)\n",
    "train_idx, valid_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=train_batch_size, sampler=train_sampler,\n",
    "    num_workers=num_workers, pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, batch_size=valid_batch_size, sampler=valid_sampler,\n",
    "    num_workers=num_workers, pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=test_batch_size, shuffle=False, \n",
    "    num_workers=num_workers, pin_memory=pin_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if optimizer_type == 'sgd':\n",
    "    optimizer = lib.IngraphGradientDescent(learning_rate=learning_rate)\n",
    "elif optimizer_type == 'momentum':\n",
    "    optimizer = lib.IngraphMomentum(learning_rate=learning_rate, momentum=momentum,\n",
    "                                    weight_decay=weight_decay, nesterov=nesterov)\n",
    "elif optimizer_type == 'rmsprop':\n",
    "    optimizer = lib.IngraphRMSProp(learning_rate=learning_rate, beta=beta, epsilon=epsilon)\n",
    "elif optimizer_type == 'adam':\n",
    "    optimizer = lib.IngraphAdam(learning_rate=learning_rate, beta2=beta2, beta1=beta1, epsilon=epsilon)\n",
    "else: \n",
    "    raise NotImplemetedError(\"This optimizer is not implemeted\")\n",
    "\n",
    "model = lib.models.fixup_resnet.FixupResNet18(num_classes=num_classes)\n",
    "maml = lib.MAML(model, model_type, optimizer=optimizer, \n",
    "    checkpoint_steps=checkpoint_steps,\n",
    "    loss_function=loss_function\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.parallel\n",
    "from concurrent.futures import Future, ThreadPoolExecutor, as_completed, TimeoutError\n",
    "GLOBAL_EXECUTOR = ThreadPoolExecutor(max_workers=16)\n",
    "\n",
    "def run_in_background(func: callable, *args, **kwargs) -> Future:\n",
    "    \"\"\" run func(*args, **kwargs) in background and return Future for its outputs \"\"\"\n",
    "    return GLOBAL_EXECUTOR.submit(func, *args, **kwargs)\n",
    "\n",
    "\n",
    "def samples_batches(dataloader, num_batches):\n",
    "    x_batches, y_batches = [], []\n",
    "    for batch_i, (x_batch, y_batch) in enumerate(dataloader):\n",
    "        if batch_i >= num_batches: break\n",
    "        x_batches.append(x_batch)\n",
    "        y_batches.append(y_batch) \n",
    "    return x_batches, y_batches\n",
    "\n",
    "\n",
    "def compute_maml_loss(maml, x_batches, y_batches, x_val_batches, y_val_batches, device):\n",
    "    with lib.training_mode(maml, is_train=True):\n",
    "        maml.resample_parameters()\n",
    "        updated_model, train_loss_history, valid_loss_history, *etc = \\\n",
    "            maml.forward(x_batches, y_batches, x_val_batches, y_val_batches, \n",
    "                         device=device, **kwargs)  \n",
    "        train_loss = torch.cat(train_loss_history[first_valid_step:]).mean()\n",
    "        valid_loss = torch.cat(valid_loss_history).mean() if len(valid_loss_history) > 0 else torch.zeros(1)\n",
    "    return train_loss, valid_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_test_loss(model, loss_function, test_loader, device='cuda'):\n",
    "    model.eval()   \n",
    "    test_loss, cls_error = 0., 0.\n",
    "    for x_test, y_test in test_loader:\n",
    "        x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "        preds = model(x_test)\n",
    "        test_loss += loss_function(preds, y_test) * x_test.shape[0]\n",
    "        cls_error += 1. * (y_test != preds.argmax(axis=-1)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    cls_error /= len(test_loader.dataset)\n",
    "    model.train()\n",
    "    return test_loss.item(), cls_error.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerResNet(lib.Trainer):\n",
    "    def train_on_batch(self, train_loader, valid_loader, prefix='train/', **kwargs):\n",
    "        \"\"\" Performs a single gradient update and reports metrics \"\"\"    \n",
    "        x_batches, y_batches = samples_batches(train_loader, maml_steps)\n",
    "        x_val_batches, y_val_batches = samples_batches(valid_loader, validation_steps)\n",
    "\n",
    "        self.meta_optimizer.zero_grad()\n",
    "        \n",
    "        train_loss, valid_loss = compute_maml_loss(self.maml, x_batches, y_batches, \n",
    "                                                   x_val_batches, y_val_batches, self.device)\n",
    "        valid_loss.backward()\n",
    "        \n",
    "        global_grad_norm = nn.utils.clip_grad_norm_(list(self.maml.initializers.parameters()), \n",
    "                                                    max_meta_parameters_grad_norm)\n",
    "        self.writer.add_scalar(prefix + \"global_grad_norm\", global_grad_norm, self.total_steps)\n",
    "        \n",
    "        for name, param in self.maml.initializers.named_parameters():\n",
    "            if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
    "                print(\"Outer Nan or inf in grads\")\n",
    "            grad_norms[name] = param.grad.norm(2).item()\n",
    "            param.grad = torch.where(torch.isnan(param.grad), torch.zeros_like(param.grad), param.grad)\n",
    "            param.grad = torch.where(torch.isinf(param.grad), torch.zeros_like(param.grad), param.grad)\n",
    "            \n",
    "        self.meta_optimizer.step()\n",
    "        self.logs.append((self.total_steps, global_grad_norm))\n",
    "        \n",
    "        return self.record(train_loss=train_loss.item(),\n",
    "                           valid_loss=valid_loss.item(), prefix=prefix)\n",
    "    \n",
    "    def parallel_train_on_batch(self, train_loader, valid_loader, prefix='train/', **kwargs):\n",
    "        # generate training/validation batches for each device\n",
    "        replica_devices = [f'cuda:{i}' for i in range(torch.cuda.device_count())]\n",
    "        replicas = torch.nn.parallel.replicate(self.maml, devices=replica_devices, detach=False)\n",
    "\n",
    "        replica_inputs = []\n",
    "        for i, (replica, replica_device) in enumerate(zip(replicas, replica_devices)):\n",
    "            replica.untrained_initializers = lib.switch_initializers_device(replica.untrained_initializers, \n",
    "                                                                            replica_device)\n",
    "\n",
    "            x_batches, y_batches = samples_batches(train_loader, maml_steps)\n",
    "            x_val_batches, y_val_batches = samples_batches(valid_loader, validation_steps)\n",
    "\n",
    "            replica_inputs.append((replica, x_batches, y_batches,\n",
    "                                   x_val_batches, y_val_batches, replica_device))\n",
    "        \n",
    "        replica_losses_futures = [run_in_background(compute_maml_loss, *replica_input)\n",
    "                                  for replica_input in replica_inputs]\n",
    "        \n",
    "        replica_losses = [future.result() for future in replica_losses_futures]\n",
    "        train_loss = sum(train_loss.item() for train_loss, _ in replica_losses) / len(replica_losses)\n",
    "        valid_loss = sum(valid_loss.to(self.device) for _, valid_loss in replica_losses) / len(replica_losses)\n",
    "        \n",
    "        self.meta_optimizer.zero_grad()\n",
    "        valid_loss.backward()\n",
    "            \n",
    "        global_grad_norm = nn.utils.clip_grad_norm_(list(self.maml.initializers.parameters()), \n",
    "                                                    max_meta_parameters_grad_norm)\n",
    "        self.writer.add_scalar(prefix + \"global_grad_norm\", global_grad_norm, self.total_steps)\n",
    "        \n",
    "        for name, param in self.maml.initializers.named_parameters():\n",
    "            if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
    "                print(\"Outer Nan or inf in grads\")\n",
    "            grad_norms[name] = param.grad.norm(2).item()\n",
    "            param.grad = torch.where(torch.isnan(param.grad), torch.zeros_like(param.grad), param.grad)\n",
    "            param.grad = torch.where(torch.isinf(param.grad), torch.zeros_like(param.grad), param.grad)\n",
    "        \n",
    "        self.meta_optimizer.step()\n",
    "        self.logs.append((self.total_steps, global_grad_norm))\n",
    "        \n",
    "        return self.record(train_loss=train_loss,\n",
    "                           valid_loss=valid_loss.item(), prefix=prefix)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def plot_pdf(self):\n",
    "        plt.figure(figsize=[22, 34])\n",
    "        i = 0\n",
    "        for name, (weight_maml_init, bias_maml_init) in self.maml.initializers.items():\n",
    "            weight_base_init, _ = self.maml.untrained_initializers[name]\n",
    "            base_mean = weight_base_init.mean.item()\n",
    "            base_std = weight_base_init.std.item()\n",
    "            maml_mean = weight_maml_init.mean.item()\n",
    "            maml_std = weight_maml_init.std.item()\n",
    "            \n",
    "            base_init = torch.distributions.Normal(base_mean, base_std)\n",
    "            maml_init = torch.distributions.Normal(maml_mean, maml_std)\n",
    "            i += 1\n",
    "            plt.subplot(6, 4, i)\n",
    "            xx = np.linspace(min([base_mean - 3.*base_std, maml_mean - 3.*maml_std]), \n",
    "                             max([base_mean + 3.*base_std, maml_mean + 3.*maml_std]), 1000)\n",
    "    \n",
    "            if i == 12:\n",
    "                yy = base_init.log_prob(torch.tensor(xx)).exp().numpy()\n",
    "                plt.plot(xx, yy, '--', label='Fixup')\n",
    "                yy = maml_init.log_prob(torch.tensor(xx)).exp().numpy()\n",
    "                plt.plot(xx, yy, c='g', label='Fixup + DIMAML')\n",
    "                leg = plt.legend(loc=4, fontsize=14.5, frameon=False)\n",
    "                for line in leg.get_lines():\n",
    "                    line.set_linewidth(1.6)\n",
    "            else:\n",
    "                yy = base_init.log_prob(torch.tensor(xx)).exp().numpy()\n",
    "                plt.plot(xx, yy, '--')\n",
    "                yy = maml_init.log_prob(torch.tensor(xx)).exp().numpy()\n",
    "                plt.plot(xx, yy, c='g')\n",
    "            \n",
    "            plt.xticks(fontsize=12)\n",
    "            plt.yticks(fontsize=12)\n",
    "            plt.title(name + '_weight', fontsize=14)\n",
    "        plt.show()\n",
    "        \n",
    "    def evaluate_metrics(self, train_loader, test_loader, prefix='val/', **kwargs):\n",
    "        \"\"\" Predicts and evaluates metrics over the entire dataset \"\"\"\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print('Baseline')\n",
    "        self.maml.resample_parameters(initializers=self.maml.untrained_initializers, is_final=True)\n",
    "        base_model = deepcopy(self.maml.model)    \n",
    "        base_train_loss_history, base_test_loss_history, base_test_error_history = \\\n",
    "            eval_model(base_model, train_loader, test_loader, epochs=1, device=self.device)\n",
    "            \n",
    "        print('Ours')\n",
    "        self.maml.resample_parameters(is_final=True)\n",
    "        maml_model = deepcopy(self.maml.model)\n",
    "        maml_train_loss_history, maml_test_loss_history, maml_test_error_history = \\\n",
    "            eval_model(maml_model, train_loader, test_loader, epochs=1, device=self.device)\n",
    "        \n",
    "        draw_plots(base_train_loss_history, base_test_loss_history, base_test_error_history, \n",
    "                   maml_train_loss_history, maml_test_loss_history, maml_test_error_history)\n",
    "        \n",
    "        self.writer.add_scalar(prefix + \"train_AUC\", sum(maml_train_loss_history), self.total_steps)\n",
    "        self.writer.add_scalar(prefix + \"test_AUC\", sum(maml_test_loss_history), self.total_steps)\n",
    "        self.writer.add_scalar(prefix + \"test_loss\", maml_test_loss_history[-1], self.total_steps)\n",
    "        self.writer.add_scalar(prefix + \"test_cls_error\", maml_test_error_history[-1], self.total_steps)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Generate Train Batch #\n",
    "########################\n",
    "            \n",
    "def generate_train_batches(train_loader, batches_in_epoch=150):\n",
    "    x_batches, y_batches = [], []\n",
    "    for batch_i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        if batch_i >= batches_in_epoch: break\n",
    "        x_batches.append(x_batch)\n",
    "        y_batches.append(y_batch)\n",
    "\n",
    "    assert len(x_batches) == len(y_batches) == batches_in_epoch\n",
    "\n",
    "    local_x = torch.cat(x_batches, dim=0)\n",
    "    local_y = torch.cat(y_batches, dim=0)\n",
    "    local_dataset = TensorDataset(local_x, local_y)\n",
    "    local_dataloader = DataLoader(local_dataset, batch_size=train_batch_size, \n",
    "                                  shuffle=True, num_workers=num_workers)\n",
    "    return local_dataloader\n",
    "        \n",
    "\n",
    "##################\n",
    "# Eval functions #\n",
    "##################\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, milestones=[30, 50]):\n",
    "    \"\"\"decrease the learning rate at 30 and 50 epoch\"\"\"\n",
    "    lr = learning_rate\n",
    "    if epoch >= milestones[0]: \n",
    "        lr /= 10\n",
    "    if epoch >= milestones[1]: \n",
    "        lr /= 10\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if param_group['initial_lr'] == learning_rate:\n",
    "            param_group['lr'] = lr\n",
    "        else:\n",
    "            if epoch < milestones[0]:\n",
    "                param_group['lr'] = param_group['initial_lr']\n",
    "            elif epoch < milestones[1]:\n",
    "                param_group['lr'] = param_group['initial_lr'] / 10.\n",
    "            else:\n",
    "                param_group['lr'] = param_group['initial_lr'] / 100.\n",
    "    return lr\n",
    "\n",
    "\n",
    "def eval_model(model, train_loader, test_loader, epochs=3, test_loss_interval=40, device='cuda'):\n",
    "    if optimizer_type == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_type == 'momentum':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, \n",
    "                                    momentum=momentum, weight_decay=weight_decay, nesterov=nesterov)\n",
    "    elif optimizer_type == 'rmsprop':\n",
    "        optimizer = torch.optim.RMSProp(parameters, lr=learning_rate, beta=beta, eps=epsilon)\n",
    "    elif optimizer_type == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, \n",
    "                                     betas=(beta1, beta2), epsilon=epsilon)\n",
    "    else: \n",
    "        raise NotImplemetedError(\"{} optimizer is not implemeted\".format(optimizer_type))\n",
    "        \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['initial_lr'] = learning_rate\n",
    "        \n",
    "    # Train loop\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "    test_error_history = []\n",
    "\n",
    "    training_mode = model.training\n",
    "    \n",
    "    total_iters = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        lr = adjust_learning_rate(optimizer, epoch)\n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(x_batch.to(device))\n",
    "            loss = loss_function(preds, y_batch.to(device))\n",
    "            loss.backward()\n",
    "            \n",
    "            grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (total_iters == 0) or (total_iters + 1) % test_loss_interval == 0:\n",
    "                train_loss_history.append(loss.item())\n",
    "                model.eval()\n",
    "                test_loss, test_error = compute_test_loss(model, loss_function, test_loader, device=device)\n",
    "                print(\"Epoch {} | Train Loss {:.4f} | Test Loss {:.4f} | Classification Error {:.4f}\"\\\n",
    "                      .format(epoch, loss.item(), test_loss, test_error))\n",
    "                test_loss_history.append(test_loss)\n",
    "                test_error_history.append(test_error)\n",
    "                model.train()\n",
    "            \n",
    "            total_iters += 1\n",
    "    \n",
    "    model.train(training_mode)\n",
    "    return train_loss_history, test_loss_history, test_error_history\n",
    "\n",
    "    \n",
    "def draw_plots(base_train_loss, base_test_loss, base_test_error,\n",
    "               maml_train_loss, maml_test_loss, maml_test_error):\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.plot(moving_average(base_train_loss, span=10), label='Baseline')\n",
    "    plt.plot(moving_average(maml_train_loss, span=10), c='g', label='Ours')\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.title(\"Train loss\", fontsize=14)\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.plot(base_test_loss, label='Baseline')\n",
    "    plt.plot(maml_test_loss, c='g', label='Ours')\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.title(\"Test loss\", fontsize=14)\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.plot(base_test_error, label='Baseline')\n",
    "    plt.plot(maml_test_error, c='g', label='Ours')\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.title(\"Test classification error\", fontsize=14)                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from pandas import DataFrame\n",
    "\n",
    "moving_average = lambda x, **kw: DataFrame({'x':np.asarray(x)}).x.ewm(**kw).mean().values\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "trainer = TrainerResNet(maml, decay_interval=meta_decay_interval, meta_lr=meta_learning_rate, \n",
    "                        meta_betas=meta_betas, exp_name=exp_name, recovery_step=recovery_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "while trainer.total_steps <= max_epochs:\n",
    "    lib.free_memory()\n",
    "    metrics = trainer.parallel_train_on_batch(\n",
    "        train_loader, valid_loader,\n",
    "        first_valid_step=first_valid_step,\n",
    "        valid_loss_interval=loss_interval, \n",
    "        loss_kwargs=loss_kwargs, \n",
    "    )\n",
    "    train_loss = metrics['train_loss']\n",
    "    train_loss_history.append(train_loss)\n",
    "    \n",
    "    valid_loss = metrics['valid_loss']\n",
    "    valid_loss_history.append(valid_loss)\n",
    "    \n",
    "    if trainer.total_steps % 10 == 0:\n",
    "        clear_output(True)\n",
    "        print(\"Step: %d | Time: %f | Train Loss %.5f | Valid loss %.5f\" \n",
    "              % (trainer.total_steps, time.time()-t0, train_loss, valid_loss))\n",
    "        plt.figure(figsize=[16, 5])\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.title('Train Loss over time')\n",
    "        plt.plot(moving_average(train_loss_history, span=50))\n",
    "        plt.scatter(range(len(train_loss_history)), train_loss_history, alpha=0.1)\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.title('Valid Loss over time')\n",
    "        plt.plot(moving_average(valid_loss_history, span=50))\n",
    "        plt.scatter(range(len(valid_loss_history)), valid_loss_history, alpha=0.1)\n",
    "        plt.show()\n",
    "        local_train_loader = generate_train_batches(train_loader, maml_n_var_batches)\n",
    "        trainer.evaluate_metrics(local_train_loader, test_loader, test_interval=20)\n",
    "        trainer.plot_pdf()\n",
    "        t0 = time.time()\n",
    "        \n",
    "    if trainer.total_steps % 100 == 0:\n",
    "        trainer.save_model()\n",
    "        \n",
    "    trainer.total_steps += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_pdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = random.randint(0, 2 ** 32 - 1)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_quotient(loss, params, eps=1e-5): \n",
    "    grad = torch.autograd.grad(loss, params, retain_graph=True, create_graph=True)\n",
    "    prod = torch.autograd.grad(sum([(g**2).sum() / 2 for g in grad]),\n",
    "                               params, retain_graph=True, create_graph=True)\n",
    "    out = sum([((g - p) / (g + eps * (2*(g >= 0).float() - 1).detach()) - 1).abs().sum() \n",
    "               for g, p in zip(grad, prod)])\n",
    "    return out / sum([p.data.nelement() for p in params])\n",
    "\n",
    "\n",
    "def metainit(model, criterion, x_size, y_size, lr=0.1, momentum=0.9, steps=200, eps=1e-5):\n",
    "    model.eval()\n",
    "    params = [p for p in model.parameters() \n",
    "              if p.requires_grad and len(p.size()) >= 2 and p.std().item() != 0]\n",
    "    memory = [0] * len(params)\n",
    "    for i in range(steps):\n",
    "        input = torch.Tensor(*x_size).normal_(0, 1).cuda()\n",
    "        target = torch.randint(0, y_size, (x_size[0],)).cuda()\n",
    "        loss = criterion(model(input), target)\n",
    "        gq = gradient_quotient(loss, list(model.parameters()), eps)\n",
    "        \n",
    "        grad = torch.autograd.grad(gq, params)\n",
    "        for j, (p, g_all) in enumerate(zip(params, grad)):\n",
    "            norm = p.data.norm().item()\n",
    "            g = torch.sign((p.data * g_all).sum() / norm) \n",
    "            memory[j] = momentum * memory[j] - lr * g.item() \n",
    "            new_norm = norm + memory[j]\n",
    "            p.data.mul_(new_norm / (norm + eps))\n",
    "        print(\"%d/GQ = %.2f\" % (i, gq.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genOrthgonal(dim):\n",
    "    a = torch.zeros((dim, dim)).normal_(0, 1)\n",
    "    q, r = torch.qr(a)\n",
    "    d = torch.diag(r, 0).sign()\n",
    "    diag_size = d.size(0)\n",
    "    d_exp = d.view(1, diag_size).expand(diag_size, diag_size)\n",
    "    q.mul_(d_exp)\n",
    "    return q\n",
    "\n",
    "def makeDeltaOrthogonal(weights, gain):\n",
    "    rows = weights.size(0)\n",
    "    cols = weights.size(1)\n",
    "    if rows < cols:\n",
    "        print(\"In_filters should not be greater than out_filters.\")\n",
    "    weights.data.fill_(0)\n",
    "    dim = max(rows, cols)\n",
    "    q = genOrthgonal(dim)\n",
    "    mid1 = weights.size(2) // 2\n",
    "    mid2 = weights.size(3) // 2\n",
    "    with torch.no_grad():\n",
    "        weights[:, :, mid1, mid2] = q[:weights.size(0), :weights.size(1)]\n",
    "        weights.mul_(gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval TinyImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/tiny-imagenet-200/'\n",
    "num_workers = {'train': 0, 'val': 0,'test': 0}\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.4802, 0.4481, 0.3975], [0.2302, 0.2265, 0.2262]),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.4802, 0.4481, 0.3975], [0.2302, 0.2265, 0.2262]),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.4802, 0.4481, 0.3975], [0.2302, 0.2265, 0.2262]),\n",
    "    ])\n",
    "}\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) \n",
    "                  for x in ['train', 'val','test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=128, shuffle=True, num_workers=num_workers[x])\n",
    "                  for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti_batches_in_epoch = len(dataloaders['train'])\n",
    "assert ti_batches_in_epoch == 782\n",
    "num_reruns = 10\n",
    "\n",
    "reruns_base_test_loss_history = []\n",
    "reruns_base_test_error_history = []\n",
    "\n",
    "reruns_metainit_test_loss_history = []\n",
    "reruns_metainit_test_error_history = []\n",
    "    \n",
    "reruns_maml_test_loss_history = []\n",
    "reruns_maml_test_error_history = []\n",
    "\n",
    "reruns_deltaorthogonal_test_loss_history = []\n",
    "reruns_deltaorthogonal_test_error_history = []\n",
    "\n",
    "for i in range(num_reruns):\n",
    "    print(f\"Rerun {i}\")\n",
    "    print(\"Ours\")\n",
    "    maml.resample_parameters(is_final=True)\n",
    "    maml_model = deepcopy(maml.model)\n",
    "\n",
    "    maml_model.fc = nn.Linear(in_features=512, out_features=200, bias=True).to(device)\n",
    "    nn.init.constant_(maml_model.fc.weight, 0)\n",
    "    nn.init.constant_(maml_model.fc.bias, 0)\n",
    "\n",
    "    maml_train_loss_history, maml_test_loss_history, maml_test_error_history = \\\n",
    "         eval_model(maml_model, dataloaders['train'], dataloaders['test'],  \n",
    "                    epochs=70, test_loss_interval=ti_batches_in_epoch, device=device)\n",
    "    \n",
    "    reruns_maml_test_loss_history.append(maml_test_loss_history)\n",
    "    reruns_maml_test_error_history.append(maml_test_error_history)\n",
    "    \n",
    "    print(\"Baseline\")\n",
    "    maml.resample_parameters(initializers=maml.untrained_initializers, is_final=True)\n",
    "    base_model = deepcopy(maml.model)\n",
    "\n",
    "    base_model.fc = nn.Linear(in_features=512, out_features=200, bias=True).to(device)\n",
    "    nn.init.constant_(base_model.fc.weight, 0)\n",
    "    nn.init.constant_(base_model.fc.bias, 0)\n",
    "\n",
    "    base_train_loss_history, base_test_loss_history, base_test_error_history = \\\n",
    "        eval_model(base_model, dataloaders['train'], dataloaders['test'], \n",
    "                   epochs=70, test_loss_interval=ti_batches_in_epoch, device=device)\n",
    "\n",
    "    reruns_base_test_loss_history.append(base_test_loss_history)\n",
    "    reruns_base_test_error_history.append(base_test_error_history)\n",
    "    \n",
    "    print(\"MetaInit\")\n",
    "    batch_x, _ = next(iter(dataloaders['train']))\n",
    "    batch_x = batch_x[:64]\n",
    "    metainit_model = lib.models.metainit_resnet.MetaInitFixupResNet18(num_classes=200).to(device)\n",
    "    metainit(metainit_model, loss_function, batch_x.shape, 200)\n",
    "    \n",
    "    metainit_train_loss_history, metainit_test_loss_history, metainit_test_error_history = \\\n",
    "        eval_model(metainit_model, dataloaders['train'], dataloaders['test'], \n",
    "                   epochs=70, test_loss_interval=ti_batches_in_epoch, device=device)\n",
    "    \n",
    "    reruns_metainit_test_loss_history.append(metainit_test_loss_history)\n",
    "    reruns_metainit_test_error_history.append(metainit_test_error_history)\n",
    "    \n",
    "    print(\"DeltaOrthogonal\")\n",
    "    deltaorthogonal_model = lib.models.FixupResNet18(num_classes=200).to(device)\n",
    "    for param in deltaorthogonal_model.parameters():\n",
    "        if len(param.size()) >= 4:\n",
    "            makeDeltaOrthogonal(param, nn.init.calculate_gain('leaky_relu'))\n",
    "    \n",
    "    deltaorthogonal_train_loss_history, deltaorthogonal_test_loss_history, deltaorthogonal_test_error_history = \\\n",
    "        eval_model(deltaorthogonal_model, dataloaders['train'], dataloaders['test'], \n",
    "                   epochs=70, test_loss_interval=ti_batches_in_epoch, device=device)\n",
    "    \n",
    "    reruns_deltaorthogonal_test_loss_history.append(deltaorthogonal_test_loss_history)\n",
    "    reruns_deltaorthogonal_test_error_history.append(deltaorthogonal_test_error_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_mean = np.array(reruns_base_test_error_history).mean(0)\n",
    "base_std = np.array(reruns_base_test_error_history).std(0, ddof=1)\n",
    "\n",
    "maml_mean = np.array(reruns_maml_test_error_history).mean(0)\n",
    "maml_std = np.array(reruns_maml_test_error_history).std(0, ddof=1)\n",
    "\n",
    "metainit_mean = np.array(reruns_metainit_test_error_history).mean(0)\n",
    "metainit_std = np.array(reruns_metainit_test_error_history).std(0, ddof=1)\n",
    "\n",
    "deltaorthogonal_mean = np.array(reruns_deltaorthogonal_test_error_history).mean(0)\n",
    "deltaorthogonal_std = np.array(reruns_deltaorthogonal_test_error_history).std(0, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'base_mean_std': (base_mean, base_std),\n",
    "            'maml_mean_std': (maml_mean, maml_std),\n",
    "            'metainit_mean_std': (metainit_mean, metainit_std),\n",
    "            'deltaorthogonal_mean_std': (deltaorthogonal_mean, deltaorthogonal_std),\n",
    "           }, \"nips_cls_errors_tinyimagenet.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "x = np.arange(0, 71, 1)\n",
    "plt.plot(x, moving_average(base_mean_relu, span=span), linewidth=1.1, label='Fixup')\n",
    "plt.fill_between(x, moving_average(base_mean_relu, span=span) - moving_average(base_std_relu, span=span), \n",
    "                 moving_average(base_mean_relu, span=span) + moving_average(base_std_relu, span=span), alpha=0.12)\n",
    "\n",
    "plt.plot(x, moving_average(delthaorthogonal_mean+0.001, span=span), linewidth=1.1, c='#7722dd', label='DeltaOrthogonal')\n",
    "\n",
    "plt.plot(x, moving_average(metainit_mean_relu, span=span), linewidth=1.1, label='MetaInit')\n",
    "plt.fill_between(x, moving_average(metainit_mean_relu, span=span) - moving_average(metainit_std_relu, span=span), \n",
    "                 moving_average(metainit_mean_relu, span=span) + moving_average(metainit_std_relu, span=span), alpha=0.12)\n",
    "\n",
    "plt.plot(x, moving_average(fixup_metainit_mean, span=span), linewidth=1.1, c='r', label='Fixup $\\\\rightarrow$ MetaInit')\n",
    "\n",
    "plt.plot(x, moving_average(maml_mean_relu, span=span), linewidth=1.1, c='g', label='Fixup $\\\\rightarrow$ DIMAML')\n",
    "plt.fill_between(x, moving_average(maml_mean_relu, span=span) - moving_average(maml_std_relu, span=span), \n",
    "                 moving_average(maml_mean_relu, span=span) + moving_average(maml_std_relu, span=span), alpha=0.12, color='g')\n",
    "plt.xlim([0, 20])\n",
    "plt.ylim([0.4, 1.])\n",
    "plt.yticks(np.arange(0.45, 1.01, 0.05), fontsize=11)\n",
    "plt.xticks(x[:71][::5], fontsize=11)\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"Classification Error\", fontsize=14)\n",
    "\n",
    "pp = PdfPages(\"nips_fixup_resnet_tiny_imagenet_darkgrid.pdf\")\n",
    "pp.savefig(bbox_inches='tight')\n",
    "pp.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "base_lr = 0.1\n",
    "batch_size = 256\n",
    "weight_decay = 1e-4\n",
    "base_learning_rate = base_lr * batch_size / 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, val_loader=None,accuracies=None):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        # switch to train mode\n",
    "        model.train()\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        inputs = inputs.cuda(non_blocking=True)\n",
    "        targets = targets.cuda(non_blocking=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, acc5 = accuracy(output, targets, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(acc1[0], inputs.size(0))\n",
    "        top5.update(acc5[0], inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        global_grad_norm = nn.utils.clip_grad_norm_(list(model.parameters()), 5)\n",
    "        print(global_grad_norm)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            accuracies.append(validate(val_loader, model, criterion).item())\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            input = input.cuda(non_blocking=True)\n",
    "            target = target.cuda(non_blocking=True)\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(acc1[0], input.size(0))\n",
    "            top5.update(acc5[0], input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            \n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    # \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    # lr = args.base_lr * (0.1 ** (epoch // 30))\n",
    "    #     for param_group in optimizer.param_groups:\n",
    "    #     param_group['lr'] = lr\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if param_group['initial_lr'] == base_learning_rate:\n",
    "            print(\"adjust non-scalar lr.\")\n",
    "            lr = base_learning_rate * (0.1 ** (epoch // 30))\n",
    "            param_group['lr'] = lr\n",
    "        else:\n",
    "            print(\"adjust scalar lr.\")\n",
    "            scalar_lr = param_group['initial_lr'] * (0.1 ** (epoch // 30))\n",
    "            param_group['lr'] = scalar_lr\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs=100):\n",
    "    accuracies = []\n",
    "    \n",
    "    best_acc1 = 0\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    parameters_bias = [p[1] for p in model.named_parameters() if 'bias' in p[0]]\n",
    "    parameters_scale = [p[1] for p in model.named_parameters() if 'scale' in p[0]]\n",
    "    parameters_others = [p[1] for p in model.named_parameters() if not ('bias' in p[0] or 'scale' in p[0])]\n",
    "    optimizer = torch.optim.SGD(\n",
    "        [{'params': parameters_bias, 'lr': base_lr/10.},\n",
    "        {'params': parameters_scale, 'lr': base_lr/10.},\n",
    "        {'params': parameters_others}],\n",
    "        lr=base_learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # Data loading code\n",
    "    traindir = os.path.join('YOUR_IMAGENET_PATH/imagenet', 'train')\n",
    "    valdir = os.path.join('YOUR_IMAGENET_PATH/imagenet', 'val')\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        traindir,\n",
    "        transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                               shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        datasets.ImageFolder(valdir, transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])),\n",
    "        batch_size=256, shuffle=False,\n",
    "        num_workers=8, pin_memory=True)\n",
    "\n",
    "    sgdr = CosineAnnealingLR(optimizer, 100, eta_min=0, last_epoch=-1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, criterion, optimizer, epoch, val_loader=val_loader, accuracies=accuracies)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        acc1 = validate(val_loader, model, criterion)\n",
    "\n",
    "        # remember best acc@1 and save checkpoint\n",
    "        is_best = acc1 > best_acc1\n",
    "        best_acc1 = max(acc1, best_acc1)\n",
    "\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_acc1': best_acc1,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "    return np.array(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reruns = 10\n",
    "\n",
    "reruns_base_accuracies = []\n",
    "reruns_maml_accuracies = []\n",
    "reruns_metainit_accuracies = []\n",
    "reruns_metainit_fixup_accuracies = []\n",
    "reruns_deltaorthogonal_accuracies = []\n",
    "\n",
    "for rerun_id in range(reruns):\n",
    "    print(f\"Rerun #{rerun_id}\")    \n",
    "    print(\"Fixup\")\n",
    "    imagenet_base_model = lib.models.ImageNetFixupResNet18(num_classes=1000).to(device)\n",
    "    base_accuracies = train_model(imagenet_base_model, epochs=3)\n",
    "    reruns_base_accuracies.append(base_accuracies)\n",
    "    lib.free_memory()\n",
    "    \n",
    "    print(\"MetaInit\")\n",
    "    imagenet_metainit_model = lib.models.MetaInitImageNetFixupResNet18(num_classes=1000).to(device)\n",
    "    metainit(imagenet_metainit_model, loss_function, torch.Size([64, 3, 224, 224]), 1000, steps=100)\n",
    "    metainit_accuracies = train_model(imagenet_metainit_model, epochs=3)\n",
    "    reruns_metainit_accuracies.append(metainit_accuracies)\n",
    "    lib.free_memory()\n",
    "    \n",
    "    print(\"Fixup --> MetaInit\")\n",
    "    imagenet_metainit_model = lib.models.ImageNetFixupResNet18(num_classes=1000).to(device)\n",
    "    metainit(imagenet_metainit_model, loss_function, torch.Size([64, 3, 224, 224]), 1000, steps=100)\n",
    "    metainit_fixup_accuracies = train_model(imagenet_metainit_model, epochs=3)\n",
    "    reruns_metainit_fixup_accuracies.append(metainit_fixup_accuracies)\n",
    "    lib.free_memory()\n",
    "    \n",
    "    print(\"Fixup --> DIMAML\")\n",
    "    imagenet_maml_model = lib.models.ImageNetFixupResNet18(num_classes=1000).to(device)\n",
    "    for name, module in imagenet_maml_model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d) and 'conv2' not in name:\n",
    "            key_name = '_'.join(name.split('.'))\n",
    "            weight_initializer = maml.initializers[key_name][0]\n",
    "            weights = weight_initializer(torch.rand_like(module.weight))\n",
    "            module.weight = nn.Parameter(weights, requires_grad=True)\n",
    "\n",
    "    maml_accuracies = train_model(imagenet_maml_model, epochs=3)\n",
    "    reruns_maml_accuracies.append(maml_accuracies)\n",
    "    lib.free_memory()\n",
    "    \n",
    "    print(\"DeltaOrthogonal\")\n",
    "    deltaorthogonal_model = lib.models.ImageNetFixupResNet18(num_classes=1000).to(device)\n",
    "    for param in deltaorthogonal_model.parameters():\n",
    "        if len(param.size()) >= 4:\n",
    "            makeDeltaOrthogonal(param, nn.init.calculate_gain('leaky_relu'))\n",
    "    \n",
    "    deltaorthogonal_accuracies = train_model(deltaorthogonal_model, epochs=3)\n",
    "    lib.free_memory()\n",
    "    \n",
    "    reruns_deltaorthogonal_accuracies.append(deltaorthogonal_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_mean_imagenet = ((100. - np.array(reruns_base_accuracies)) / 100).mean(0)\n",
    "base_std_imagenet = ((100. - np.array(reruns_base_accuracies)) / 100).std(0, ddof=1)\n",
    "\n",
    "metainit_mean_imagenet = ((100. - np.array(reruns_metainit_accuracies)) / 100).mean(0)\n",
    "metainit_std_imagenet = ((100. - np.array(reruns_metainit_accuracies)) / 100).std(0, ddof=1)\n",
    "\n",
    "maml_mean_imagenet = ((100. - np.array(reruns_maml_accuracies)) / 100).mean(0)\n",
    "maml_std_imagenet = ((100. - np.array(reruns_maml_accuracies)) / 100).std(0, ddof=1)\n",
    "\n",
    "metainit_fixup_mean_imagenet = ((100. - np.array(reruns_metainit_fixup_accuracies)) / 100).mean(0)\n",
    "metainit_fixup_std_imagenet = ((100. - np.array(reruns_metainit_fixup_accuracies)) / 100).std(0, ddof=1)\n",
    "\n",
    "deltaorthogonal_mean_imagenet = ((100. - np.array(reruns_deltaorthogonal_accuracies)) / 100).mean(0)\n",
    "deltaorthogonal_std_imagenet = ((100. - np.array(reruns_deltaorthogonal_accuracies)) / 100).std(0, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "span = 5\n",
    "x = np.arange(0, 33, 1)\n",
    "\n",
    "plt.plot(x, moving_average(base_mean_imagenet, span=span), linewidth=1.1, label='Fixup')\n",
    "plt.fill_between(x, moving_average(base_mean_imagenet, span=span) - moving_average(base_std_imagenet, span=span), \n",
    "                 moving_average(base_mean_imagenet, span=span) + moving_average(base_std_imagenet, span=span), alpha=0.12)\n",
    "\n",
    "plt.plot(x, moving_average(deltaorthogonal_mean_imagenet, span=span), linewidth=1.1, c='#7722dd', label='DeltaOrthogonal')\n",
    "plt.fill_between(x, moving_average(deltaorthogonal_mean_imagenet, span=span) - moving_average(deltaorthogonal_std_imagenet, span=span), \n",
    "                 moving_average(deltaorthogonal_mean_imagenet, span=span) + moving_average(deltaorthogonal_std_imagenet, span=span), color='#7722dd', alpha=0.12)\n",
    "\n",
    "plt.plot(x, moving_average(metainit_mean_imagenet, span=span), linewidth=1.1, label='MetaInit')\n",
    "plt.fill_between(x, moving_average(metainit_mean_imagenet, span=span) - moving_average(metainit_std_imagenet, span=span), \n",
    "                 moving_average(metainit_mean_imagenet, span=span) + moving_average(metainit_std_imagenet, span=span), alpha=0.12)\n",
    "\n",
    "plt.plot(x, moving_average(metainit_fixup_mean_imagenet, span=span), linewidth=1.1, c='r', label='Fixup $\\\\rightarrow$ MetaInit')\n",
    "plt.fill_between(x, moving_average(metainit_fixup_mean_imagenet, span=span) - moving_average(metainit_fixup_std_imagenet, span=span), \n",
    "                 moving_average(metainit_fixup_mean_imagenet, span=span) + moving_average(metainit_fixup_std_imagenet, span=span), alpha=0.12, color='r')\n",
    "\n",
    "plt.plot(x, moving_average(maml_mean_imagenet, span=span), linewidth=1.1, c='g', label='Fixup $\\\\rightarrow$ DIMAML')\n",
    "plt.fill_between(x, moving_average(maml_mean_imagenet, span=span) - moving_average(maml_std_imagenet, span=span), \n",
    "                 moving_average(maml_mean_imagenet, span=span) + moving_average(maml_std_imagenet, span=span), alpha=0.12, color='g')\n",
    "\n",
    "\n",
    "plt.xlim([0, 32])\n",
    "plt.ylim([0.58, 1.])\n",
    "plt.yticks(np.arange(0.6, 1.01, 0.05), fontsize=11)\n",
    "plt.xticks(np.arange(0, 31, 3), np.arange(0, 15151, 1500), fontsize=11)\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "# plt.title(\"Test Classification Error\", fontsize=14)\n",
    "plt.xlabel(\"Iteration\", fontsize=14)\n",
    "plt.ylabel(\"Classification Error\", fontsize=14)\n",
    "\n",
    "# plt.gca().set_facecolor('xkcd:salmon')\n",
    "# plt.gca().set_facecolor((1.0, 229/255., 204/255.))\n",
    "# plt.gcf().set_size_inches(plt.gcf().get_size_inches()[1], plt.gcf().get_size_inches()[1])\n",
    "\n",
    "pp = PdfPages(\"nips_fixup_resnet_imagenet_darkgrid.pdf\")\n",
    "pp.savefig(bbox_inches='tight')\n",
    "pp.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'base':reruns_base_accuracies,\n",
    "            'maml':reruns_maml_accuracies, \n",
    "            'metainit':reruns_metainit_accuracies,\n",
    "            'metainit_fixup':reruns_metainit_fixup_accuracies, \n",
    "            'deltaorthogonal':reruns_deltaorthogonal_accuracies}, 'nips_imagenet_accuracies.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
