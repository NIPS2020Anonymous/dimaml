{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "import os, sys, time\n",
    "sys.path.insert(0, '..')\n",
    "import lib\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# For reproducibility\n",
    "import random\n",
    "seed = random.randint(0, 2**32 - 1)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'lstm'\n",
    "\n",
    "# Language Model\n",
    "emb_size = 128\n",
    "sequence_length = 100\n",
    "hidden_size = 256\n",
    "\n",
    "# Dataset \n",
    "data_dir = './data'\n",
    "train_batch_size = 128\n",
    "valid_batch_size = 128\n",
    "test_batch_size = 128\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "loss_function = F.nll_loss\n",
    "\n",
    "# MAML\n",
    "max_epochs = 3000\n",
    "maml_batches_in_epoch = 200\n",
    "maml_epochs = 1\n",
    "maml_steps = maml_epochs * maml_batches_in_epoch\n",
    "\n",
    "max_meta_parameters_grad_norm = 10.\n",
    "\n",
    "loss_interval = 40\n",
    "first_valid_step = 40\n",
    "\n",
    "assert (maml_steps - first_valid_step) % loss_interval == 0\n",
    "validation_steps = int((maml_steps - first_valid_step) / loss_interval + 1)\n",
    "\n",
    "# Optimizer\n",
    "optimizer_type='adam'\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "\n",
    "#Adam\n",
    "beta1 = momentum\n",
    "beta2 = 0.999\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Meta optimizer\n",
    "meta_learning_rate = 0.0003\n",
    "meta_betas = (0.9, 0.997)\n",
    "meta_decay_interval = max_epochs\n",
    "\n",
    "checkpoint_steps = 6\n",
    "recovery_step = None\n",
    "\n",
    "\n",
    "kwargs = dict(\n",
    "    first_valid_step=first_valid_step,\n",
    "    valid_loss_interval=loss_interval,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = f\"LSTM-2_LM_{model_type}_PTb_{optimizer_type}_lr{learning_rate}\" + \\\n",
    "           f\"_meta_lr{meta_learning_rate}_steps{maml_steps}_interval{loss_interval}\" + \\\n",
    "           f\"_tr_bs{train_batch_size}_val_bs{valid_batch_size}_seed_{seed}\"\n",
    "\n",
    "print(\"Experiment name: \", exp_name)\n",
    "\n",
    "logs_path = \"./logs/{}\".format(exp_name)\n",
    "assert recovery_step is not None or not os.path.exists(logs_path)\n",
    "# !rm -rf {logs_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "data_dir = 'data/'\n",
    "\n",
    "PTb_TEXT = torchtext.data.Field(lower=False, tokenize=list)\n",
    "PTb_train, PTb_valid, PTb_test = torchtext.datasets.PennTreebank.splits(PTb_TEXT, root=data_dir)\n",
    "PTb_TEXT.build_vocab(PTb_train)\n",
    "\n",
    "PTb_voc_size = len(PTb_TEXT.vocab)\n",
    "\n",
    "PTb_train_loader = torchtext.data.BPTTIterator(PTb_train, train_batch_size, sequence_length, \n",
    "                                           train=True, device=device, repeat=True, shuffle=True)\n",
    "PTb_valid_loader = torchtext.data.BPTTIterator(PTb_valid, valid_batch_size, sequence_length, \n",
    "                                           train=True, device=device, repeat=True, shuffle=True)\n",
    "\n",
    "PTb_test_ids = list(map(PTb_TEXT.vocab.stoi.get, PTb_test.examples[0].text))\n",
    "PTb_test_ids = list(map(lambda x: x if x is not None else 0, PTb_test_ids))\n",
    "full_PTb_test_ids = torch.tensor(PTb_test_ids)\n",
    "part_PTb_test_ids = torch.tensor(PTb_test_ids)[:len(PTb_test_ids) // 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.parallel\n",
    "from concurrent.futures import Future, ThreadPoolExecutor, as_completed, TimeoutError\n",
    "GLOBAL_EXECUTOR = ThreadPoolExecutor(max_workers=16)\n",
    "\n",
    "def run_in_background(func: callable, *args, **kwargs) -> Future:\n",
    "    \"\"\" run func(*args, **kwargs) in background and return Future for its outputs \"\"\"\n",
    "    return GLOBAL_EXECUTOR.submit(func, *args, **kwargs)\n",
    "\n",
    "\n",
    "def samples_batches(dataloader, num_batches):\n",
    "    x_batches, y_batches = [], []\n",
    "    for batch_i, batch in enumerate(dataloader):\n",
    "        if batch_i >= num_batches: break\n",
    "        x_batches.append(batch.text.t())\n",
    "        y_batches.append(batch.target.t()) \n",
    "    return x_batches, y_batches\n",
    "\n",
    "\n",
    "def compute_maml_loss(_maml, x_batches, y_batches, x_val_batches, y_val_batches, device):\n",
    "    with lib.training_mode(maml, is_train=True):\n",
    "        maml.resample_parameters()\n",
    "        updated_model, train_loss_history, valid_loss_history, *etc = \\\n",
    "            maml.forward(x_batches, y_batches, x_val_batches, y_val_batches, \n",
    "                         device=device, **kwargs)  \n",
    "        train_loss = torch.cat(train_loss_history[first_valid_step:]).mean()\n",
    "        valid_loss = torch.cat(valid_loss_history).mean() if len(valid_loss_history) > 0 else torch.zeros(1)\n",
    "    return train_loss, valid_loss\n",
    "\n",
    "def compute_loss(logp_next, batch_targets, **kwargs): # logp_next -> batch_inputs\n",
    "    #logp_next = model(batch_inputs)  # [batch_size, seq_length, voc_size]\n",
    "    xent = F.nll_loss(logp_next.reshape(-1, logp_next.shape[-1]), \n",
    "                      batch_targets.reshape(-1), reduction='none')\n",
    "    loss = xent.mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_test_loss(model, loss_function, test_ids, **kwargs):\n",
    "    model = model.cpu()\n",
    "    logp_next = model(test_ids[:-1][None])\n",
    "    loss = loss_function(logp_next, test_ids[1:][None])\n",
    "    model = model.to(device)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if optimizer_type == 'sgd':\n",
    "    optimizer = lib.IngraphGradientDescent(learning_rate=learning_rate)\n",
    "elif optimizer_type == 'momentum':\n",
    "    optimizer = lib.IngraphMomentum(learning_rate=learning_rate, momentum=momentum)\n",
    "elif optimizer_type == 'rmsprop':\n",
    "    optimizer = lib.IngraphRMSProp(learning_rate=learning_rate, momentum=momentum, alpha=0.99, epsilon=epsilon)\n",
    "elif optimizer_type == 'adam':\n",
    "    optimizer = lib.IngraphAdam(learning_rate=learning_rate, beta1=beta1, beta2=beta2, epsilon=epsilon)\n",
    "else: \n",
    "    raise NotImplemetedError(\"This optimizer is not implemeted\")\n",
    "\n",
    "model = lib.models.language_model.LanguageModel(PTb_voc_size, emb_size, hid_size=hidden_size)\n",
    "maml = lib.MAML(model, model_type, optimizer=optimizer, \n",
    "    checkpoint_steps=checkpoint_steps,\n",
    "    loss_function=compute_loss\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerLM(lib.Trainer):\n",
    "    def train_on_batch(self, train_loader, valid_loader, prefix='train/', **kwargs):\n",
    "        \"\"\" Performs a single gradient update and reports metrics \"\"\"\n",
    "        # Prepare train data\n",
    "        x_batches, y_batches = samples_batches(train_loader, maml_batches_in_epoch)\n",
    "        \n",
    "        # Due to a little amount of validation data, \n",
    "        # validation batches are sampled from both remained train and valid sets\n",
    "        x_val_batches, y_val_batches = samples_batches(train_loader, validation_steps - 1)\n",
    "        x_tmp_batches, y_tmp_batches = samples_batches(valid_loader, 1)\n",
    "        x_val_batches.extend(x_tmp_batches)\n",
    "        y_val_batches.extend(y_tmp_batches)\n",
    "        \n",
    "        # Perform step\n",
    "        self.meta_optimizer.zero_grad()\n",
    "        train_loss, valid_loss = compute_maml_loss(self.maml, x_batches, y_batches, \n",
    "                                                   x_val_batches, y_val_batches, self.device)\n",
    "        valid_loss.backward()\n",
    "        \n",
    "        global_grad_norm = nn.utils.clip_grad_norm_(list(self.maml.initializers.parameters()), \n",
    "                                                    max_meta_parameters_grad_norm)\n",
    "        self.writer.add_scalar(prefix + \"global_grad_norm\", global_grad_norm, self.total_steps)\n",
    "        \n",
    "        grad_norms = {}\n",
    "        for name, param in self.maml.initializers.named_parameters():\n",
    "            if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
    "                print(\"Nan or inf in meta grads\")\n",
    "            grad_norms[name] = param.grad.norm(2).item()\n",
    "            param.grad = torch.where(torch.isnan(param.grad), torch.zeros_like(param.grad), param.grad)\n",
    "            param.grad = torch.where(torch.isinf(param.grad), torch.zeros_like(param.grad), param.grad)\n",
    "            \n",
    "        self.meta_optimizer.step()\n",
    "        self.logs.append((self.total_steps, train_loss.item(), valid_loss.item(), global_grad_norm, grad_norms))\n",
    "        return self.record(train_loss=train_loss.item(),\n",
    "                           valid_loss=valid_loss.item(), prefix=prefix)\n",
    "    \n",
    "    \n",
    "    def parallel_train_on_batch(self, train_loader, valid_loader, prefix='train/', **kwargs):\n",
    "        # generate training/validation batches for each device\n",
    "        replica_devices = [f'cuda:{i}' for i in range(torch.cuda.device_count())]\n",
    "        replicas = torch.nn.parallel.replicate(self.maml, devices=replica_devices, detach=False)\n",
    "\n",
    "        replica_inputs = []\n",
    "        for i, (replica, replica_device) in enumerate(zip(replicas, replica_devices)):\n",
    "            replica.untrained_initializers = lib.switch_initializers_device(replica.untrained_initializers, \n",
    "                                                                        replica_device)\n",
    "\n",
    "            x_batches, y_batches = [], []\n",
    "            for _ in range(maml_epochs):\n",
    "                x, y = samples_batches(train_loader, batches_in_epoch)\n",
    "                x_batches.extend(x)\n",
    "                y_batches.extend(y)\n",
    "            \n",
    "            x_val_batches, y_val_batches = samples_batches(valid_loader, validation_steps)\n",
    "\n",
    "            replica_inputs.append((replica, x_batches, x_batches,\n",
    "                                   x_val_batches, x_val_batches, replica_device))\n",
    "        \n",
    "        replica_losses_futures = [run_in_background(compute_maml_loss, *replica_input)\n",
    "                                  for replica_input in replica_inputs]\n",
    "        \n",
    "        replica_losses = [future.result() for future in replica_losses_futures]\n",
    "        train_loss = sum(train_loss.item() for train_loss, _ in replica_losses) / len(replica_losses)\n",
    "        valid_loss = sum(valid_loss.to(self.device) for _, valid_loss in replica_losses) / len(replica_losses)\n",
    "        \n",
    "        self.meta_optimizer.zero_grad()\n",
    "        valid_loss.backward()\n",
    "        \n",
    "        global_grad_norm = nn.utils.clip_grad_norm_(list(self.maml.initializers.parameters()), \n",
    "                                                    max_meta_parameters_grad_norm)\n",
    "        self.writer.add_scalar(prefix + \"global_grad_norm\", global_grad_norm, self.total_steps)\n",
    "        \n",
    "        grad_norms = {}\n",
    "        for name, param in self.maml.initializers.named_parameters():\n",
    "            if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
    "                print(\"Nan or inf in meta grads\")\n",
    "            grad_norms[name] = param.grad.norm(2).item()\n",
    "            param.grad = torch.where(torch.isnan(param.grad), torch.zeros_like(param.grad), param.grad)\n",
    "            param.grad = torch.where(torch.isinf(param.grad), torch.zeros_like(param.grad), param.grad)\n",
    "        \n",
    "        self.meta_optimizer.step()\n",
    "        self.logs.append((self.total_steps, train_loss.item(), valid_loss.item(), global_grad_norm, grad_norms))\n",
    "        \n",
    "        return self.record(train_loss=train_loss,\n",
    "                           valid_loss=valid_loss.item(), prefix=prefix)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def plot_pdf(self):\n",
    "        indices = [-1, 1, 2, 3, 4, 5, 7, 8, 9, 10, 12, 13, 14, 15, 6, \n",
    "                   17, 18, 19, 20, 22, 23, 24, 25, 27, 28, 29, 30]\n",
    "        plt.figure(figsize=[22, 26])\n",
    "        i = 0\n",
    "        for name, (weight_maml_init, bias_maml_init) in self.maml.initializers.items():\n",
    "            weight_base_init, bias_base_init = self.maml.untrained_initializers[name]\n",
    "            if not isinstance(weight_maml_init, nn.ModuleDict):\n",
    "                weight_base_mean = weight_base_init.mean.item()\n",
    "                weight_base_std = weight_base_init.std.item()\n",
    "                weight_base_init = torch.distributions.Normal(weight_base_mean, weight_base_std)\n",
    "\n",
    "                weight_maml_mean = weight_maml_init.mean.item()\n",
    "                weight_maml_std = weight_maml_init.std.item()\n",
    "                weight_maml_init = torch.distributions.Normal(weight_maml_mean, weight_maml_std)\n",
    "\n",
    "                xx = np.linspace(min([weight_base_mean - 3.*weight_base_std, \n",
    "                                      weight_maml_mean - 3.*weight_maml_std]), \n",
    "                                 max([weight_base_mean + 3.*weight_base_std, \n",
    "                                      weight_maml_mean + 3.*weight_maml_std]), 1000)\n",
    "                i += 1\n",
    "                plt.subplot(6, 5, indices[i])\n",
    "                yy = weight_base_init.log_prob(torch.tensor(xx)).exp().numpy()\n",
    "                plt.plot(xx, yy, '--')\n",
    "                yy = weight_maml_init.log_prob(torch.tensor(xx)).exp().numpy()\n",
    "                plt.plot(xx, yy, c='g')\n",
    "                plt.xticks(fontsize=12)\n",
    "                plt.yticks(fontsize=12)\n",
    "                plt.title(name + '_weight', fontsize=14)\n",
    "\n",
    "                if bias_maml_init is not None:\n",
    "                    bias_base_mean = bias_base_init.mean.item()\n",
    "                    bias_base_std = bias_base_init.std.item()\n",
    "                    bias_base_init = torch.distributions.Normal(bias_base_mean, bias_base_std)\n",
    "\n",
    "                    bias_maml_mean = bias_maml_init.mean.item()\n",
    "                    bias_maml_std = bias_maml_init.std.item()\n",
    "                    bias_maml_init = torch.distributions.Normal(bias_maml_mean, bias_maml_std)\n",
    "\n",
    "                    i += 1\n",
    "                    plt.subplot(6, 5, indices[i])\n",
    "                    if i == 12:\n",
    "                        yy = bias_base_init.log_prob(torch.tensor(xx)).exp().numpy()\n",
    "                        plt.plot(xx, yy, '--', label='Kaiming')\n",
    "                        yy = bias_maml_init.log_prob(torch.tensor(xx)).exp().numpy()\n",
    "                        plt.plot(xx, yy, c='g', label='DIMAML')\n",
    "                        leg = plt.legend(loc=4, fontsize=15, frameon=False)\n",
    "                        for line in leg.get_lines():\n",
    "                            line.set_linewidth(1.6)\n",
    "                    else:\n",
    "                        yy = bias_base_init.log_prob(torch.tensor(xx)).exp().numpy()\n",
    "                        plt.plot(xx, yy, '--')\n",
    "                        yy = bias_maml_init.log_prob(torch.tensor(xx)).exp().numpy()\n",
    "                        plt.plot(xx, yy, c='g')\n",
    "\n",
    "                    plt.title(name + '_bias', fontsize=14)\n",
    "                    plt.yticks(fontsize=12)\n",
    "                    plt.xticks(fontsize=12)\n",
    "            else:\n",
    "                for weight_name, maml_init in weight_maml_init.items():\n",
    "                    base_init = weight_base_init[weight_name]\n",
    "                    \n",
    "                    weight_name_split = weight_name.split('_')\n",
    "                    if weight_name_split[-1] == '0':\n",
    "                        gate_name = 'input_gate'\n",
    "                    elif weight_name_split[-1] == '1':\n",
    "                        gate_name = 'forget_gate'\n",
    "                    elif weight_name_split[-1] == '2':\n",
    "                        gate_name = 'update'\n",
    "                    elif weight_name_split[-1] == '3':\n",
    "                        gate_name = 'output_gate'\n",
    "                        \n",
    "                    weight_base_mean = base_init.mean.item()\n",
    "                    weight_base_std = base_init.std.item()\n",
    "                    base_init = torch.distributions.Normal(weight_base_mean, weight_base_std)\n",
    "\n",
    "                    weight_maml_mean = maml_init.mean.item()\n",
    "                    weight_maml_std = maml_init.std.item()\n",
    "                    maml_init = torch.distributions.Normal(weight_maml_mean, weight_maml_std)\n",
    "\n",
    "                    xx = np.linspace(min([weight_base_mean - 3.*weight_base_std, \n",
    "                                      weight_maml_mean - 3.*weight_maml_std]), \n",
    "                             max([weight_base_mean + 3.*weight_base_std, \n",
    "                                  weight_maml_mean + 3.*weight_maml_std]), 1000)\n",
    "                    i += 1\n",
    "                    plt.subplot(6, 5, indices[i])\n",
    "                    yy = base_init.log_prob(torch.tensor(xx)).exp().numpy()\n",
    "                    plt.plot(xx, yy, '--')\n",
    "                    yy = maml_init.log_prob(torch.tensor(xx)).exp().numpy()\n",
    "                    plt.plot(xx, yy, c='g')\n",
    "                    plt.xticks(fontsize=12)\n",
    "                    plt.yticks(fontsize=12)\n",
    "\n",
    "                    if weight_name_split[0] == 'weight':\n",
    "                        weight_name = '_'.join(['lstm', gate_name] + weight_name_split[:2])\n",
    "                    else:\n",
    "                        weight_name = '_'.join(['lstm', gate_name, weight_name_split[0]])\n",
    "                    plt.title(weight_name, fontsize=14)               \n",
    "        plt.show()\n",
    "        \n",
    "    def evaluate_metrics(self, train_loader, test_loader, prefix='val/', **kwargs):\n",
    "        \"\"\" Predicts and evaluates metrics over the entire dataset \"\"\"\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print('Baseline')\n",
    "        self.maml.resample_parameters(initializers=self.maml.untrained_initializers, is_final=True)\n",
    "        base_model = deepcopy(self.maml.model)    \n",
    "        base_train_loss_history, base_test_loss_history = eval_model(base_model, train_loader, test_loader,\n",
    "                                                                     device=self.device, **kwargs)\n",
    "        print('Ours')\n",
    "        self.maml.resample_parameters(is_final=True)\n",
    "        maml_model = deepcopy(self.maml.model)\n",
    "        maml_train_loss_history, maml_test_loss_history = eval_model(maml_model, train_loader, test_loader, \n",
    "                                                                     device=self.device,  **kwargs)\n",
    "        draw_plots(base_train_loss_history, base_test_loss_history, \n",
    "                   maml_train_loss_history, maml_test_loss_history)\n",
    "        \n",
    "        self.writer.add_scalar(prefix + \"train_AUC\", sum(maml_train_loss_history), self.total_steps)\n",
    "        self.writer.add_scalar(prefix + \"test_AUC\", sum(maml_test_loss_history), self.total_steps)\n",
    "        self.writer.add_scalar(prefix + \"test_loss\", maml_test_loss_history[-1], self.total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Eval functions #\n",
    "##################\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, milestone=80, **kwargs):\n",
    "    \"\"\"decrease the learning rate at 80 epoch\"\"\"\n",
    "    if milestone <= epoch:\n",
    "        lr = learning_rate / 10.\n",
    "    else:\n",
    "        lr = learning_rate\n",
    "        \n",
    "    for param_group in optimizer.param_groups:        \n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def eval_model(model, train_loader, test_ids, epochs=3, \n",
    "               test_loss_interval=20, mode='train', device='cuda', **kwargs):\n",
    "    if optimizer_type == 'sgd':\n",
    "        optimizer = torch.optim.SGD(maml.get_parameters(model), lr=learning_rate)\n",
    "    elif optimizer_type == 'momentum':\n",
    "        optimizer = torch.optim.SGD(maml.get_parameters(model), lr=learning_rate, momentum=momentum)\n",
    "    elif optimizer_type == 'rmsprop':\n",
    "        optimizer = torch.optim.RMSprop(maml.get_parameters(model), lr=learning_rate, beta=beta, eps=epsilon)\n",
    "    elif optimizer_type == 'adam':\n",
    "        optimizer = torch.optim.Adam(maml.get_parameters(model), lr=learning_rate, \n",
    "                                     betas=(beta1, beta2), eps=epsilon)\n",
    "    else: \n",
    "        raise NotImplemetedError(\"{} optimizer is not implemeted\".format(optimizer_type))\n",
    "        \n",
    "    # Train loop\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "\n",
    "    training_mode = model.training\n",
    "    total_iters = 0\n",
    "    epoch = 0\n",
    "    model.train()\n",
    "            \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        adjust_learning_rate(optimizer, epoch, **kwargs)\n",
    "        epoch = (total_iters + 1) // len(train_loader)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(batch.text.t())\n",
    "        loss = compute_loss(preds, batch.target.t())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_history.append(loss.item())\n",
    "        \n",
    "        if (total_iters == 0) or (total_iters + 1) % test_loss_interval == 0:\n",
    "            model.eval()\n",
    "            test_loss = compute_test_loss(model, compute_loss, test_ids, device=device, **kwargs)\n",
    "            bpc = test_loss * math.log2(math.e)\n",
    "            print(f\"Epoch {epoch} | Iteration {total_iters + 1} | Loss {test_loss} | bpc {bpc}\")\n",
    "            test_loss_history.append(test_loss)\n",
    "            model.train()\n",
    "            \n",
    "        if epoch >= epochs: break\n",
    "        if mode == 'train' and total_iters >= maml_steps: break\n",
    "        total_iters += 1\n",
    "        \n",
    "    model.train(training_mode)\n",
    "    return train_loss_history, test_loss_history\n",
    "\n",
    "    \n",
    "def draw_plots(base_train_loss, base_test_loss, maml_train_loss, maml_test_loss):\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(moving_average(base_train_loss, span=10), label='Baseline')\n",
    "    plt.plot(moving_average(maml_train_loss, span=10), c='g', label='Ours')\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.title(\"Train loss\", fontsize=14)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(base_test_loss, label='Baseline')\n",
    "    plt.plot(maml_test_loss, c='g', label='Ours')\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.title(\"Test loss\", fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from pandas import DataFrame\n",
    "\n",
    "moving_average = lambda x, **kw: DataFrame({'x':np.asarray(x)}).x.ewm(**kw).mean().values\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "trainer = TrainerLM(maml, decay_interval=meta_decay_interval, meta_lr=meta_learning_rate, \n",
    "                    meta_betas=meta_betas, exp_name=exp_name, recovery_step=recovery_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.free_memory()\n",
    "t0 = time.time()\n",
    "\n",
    "while trainer.total_steps <= max_epochs:\n",
    "    metrics = trainer.train_on_batch(\n",
    "        PTb_train_loader, PTb_valid_loader, \n",
    "        first_valid_step=first_valid_step, \n",
    "        valid_loss_interval=loss_interval,\n",
    "    )\n",
    "    \n",
    "    train_loss = metrics['train_loss']\n",
    "    train_loss_history.append(train_loss)\n",
    "    \n",
    "    valid_loss = metrics['valid_loss']\n",
    "    valid_loss_history.append(valid_loss)\n",
    "    \n",
    "    if trainer.total_steps % 20 == 0:\n",
    "        clear_output(True)\n",
    "        print(\"Step %d | Time: %f | Train Loss %.5f | Valid loss %.5f\" % \n",
    "              (trainer.total_steps, time.time()-t0, train_loss, valid_loss))\n",
    "        plt.figure(figsize=[16, 5])\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.title('Train Loss over time')\n",
    "        plt.plot(moving_average(train_loss_history, span=50))\n",
    "        plt.scatter(range(len(train_loss_history)), train_loss_history, alpha=0.1)\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.title('Valid Loss over time')\n",
    "        plt.plot(moving_average(valid_loss_history, span=50))\n",
    "        plt.scatter(range(len(valid_loss_history)), valid_loss_history, alpha=0.1)\n",
    "        plt.show()\n",
    "        trainer.evaluate_metrics(PTb_train_loader, part_PTb_test_ids, \n",
    "                                 batches_in_epoch=maml_batches_in_epoch,\n",
    "                                 epochs=maml_epochs, test_loss_interval=loss_interval)\n",
    "        trainer.plot_pdf()\n",
    "        t0 = time.time()\n",
    "    \n",
    "    if trainer.total_steps % 100 == 0:\n",
    "        trainer.save_model()\n",
    "    trainer.total_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_pdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def gradient_quotient(loss, params, eps=1e-5): \n",
    "    grad = torch.autograd.grad(loss, params, retain_graph=True, create_graph=True)\n",
    "    prod = torch.autograd.grad(sum([(g**2).sum() / 2 for g in grad]),\n",
    "                               params, retain_graph=True, create_graph=True)\n",
    "    out = sum([((g - p) / (g + eps * (2*(g >= 0).float() - 1).detach()) - 1).abs().sum() \n",
    "               for g, p in zip(grad, prod)])\n",
    "    return out / sum([p.data.nelement() for p in params])\n",
    "\n",
    "              \n",
    "def metainit(model, criterion, x_size, y_size, lr=0.1, momentum=0.9, steps=150, eps=1e-5):\n",
    "    model.eval()\n",
    "    params = [p for p in model.parameters() \n",
    "              if p.requires_grad and len(p.size()) >= 2]\n",
    "    memory = [0] * len(params)\n",
    "    for i in range(steps):\n",
    "        sequences = torch.randint(0, y_size, torch.Size([x_size[0], x_size[1] + 1])).cuda()\n",
    "        input, target = sequences[:, :-1], sequences[:, 1:]\n",
    "        loss = criterion(model(input), target)\n",
    "        gq = gradient_quotient(loss, [p for p in model.parameters() \n",
    "              if p.requires_grad], eps)\n",
    "        \n",
    "        grad = torch.autograd.grad(gq, params)\n",
    "        for j, (p, g_all) in enumerate(zip(params, grad)):\n",
    "            norm = p.data.norm().item()\n",
    "            g = torch.sign((p.data * g_all).sum() / norm) \n",
    "            memory[j] = momentum * memory[j] - lr * g.item() \n",
    "            new_norm = norm + memory[j]\n",
    "            p.data.mul_(new_norm / norm)\n",
    "        print(\"%d/GQ = %.2f\" % (i, gq.item()))\n",
    "              \n",
    "              \n",
    "def lstm_orthogonal(cell, gain=1):\n",
    "    cell.reset_parameters()\n",
    "\n",
    "    # orthogonal initialization of recurrent weights\n",
    "    _, hh, _, _ = list(cell.parameters())\n",
    "    for i in range(0, hh.size(0), cell.hidden_size):\n",
    "         torch.nn.init.orthogonal_(hh[i:i + cell.hidden_size], gain=gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval WikiText2 (D_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WikiTEXT = torchtext.data.Field(lower=False, tokenize=list)\n",
    "\n",
    "# load corpora, each dataset only contains one long \"example\" with all text in that example\n",
    "wikitext2_train, wikitext2_valid, wikitext2_test = torchtext.datasets.WikiText2.splits(WikiTEXT, root=data_dir, \n",
    "                                                         train='wiki.train.raw',\n",
    "                                                         validation='wiki.valid.raw',\n",
    "                                                         test='wiki.test.raw')\n",
    "WikiTEXT.build_vocab(wikitext2_train)\n",
    "wikitext2_voc_size = len(WikiTEXT.vocab)\n",
    "\n",
    "wikitext2_train_loader = torchtext.data.BPTTIterator(wikitext2_train, train_batch_size, sequence_length, \n",
    "                                           train=True, device=device, repeat=True, shuffle=True)\n",
    "wikitext2_valid_loader = torchtext.data.BPTTIterator(wikitext2_valid, valid_batch_size, sequence_length, \n",
    "                                           train=False, device=device, repeat=False, shuffle=True)\n",
    "\n",
    "wikitext2_test_ids = list(map(WikiTEXT.vocab.stoi.get, wikitext2_test.examples[0].text))\n",
    "wikitext2_test_ids = list(map(lambda x: x if x is not None else 0, wikitext2_test_ids))\n",
    "full_wikitext2_test_ids = torch.tensor(wikitext2_test_ids)\n",
    "\n",
    "# For MetaInit\n",
    "batch = next(iter(wikitext2_train_loader))\n",
    "text, target = batch.text.t(), batch.target.t()\n",
    "\n",
    "# Tune voc_size\n",
    "maml.model.emb_vectors = nn.Embedding(wikitext2_voc_size, emb_size).to(device)\n",
    "maml.model.logits = nn.Linear(hidden_size, wikitext2_voc_size).to(device)\n",
    "maml.model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reruns = 10\n",
    "wikitext2_batches_in_epoch = len(wikitext2_train_loader)\n",
    "\n",
    "reruns_base, reruns_orthogonal = [], []\n",
    "reruns_metainit, reruns_dimaml = [], []\n",
    "\n",
    "for rerun_id in range(num_reruns):\n",
    "    print(f\"Rerun #{rerun_id}\")\n",
    "    print('DIMAML')\n",
    "    maml.resample_parameters(is_final=True)\n",
    "    maml_model = deepcopy(maml.model)\n",
    "    maml_train_loss_history, maml_test_loss_history = eval_model(maml_model, wikitext2_train_loader,\n",
    "                                                                 part_wikitext2_test_ids, epochs=100, \n",
    "                                                                 device=device, mode='eval',\n",
    "                                                                 test_loss_interval=10*wikitext2_batches_in_epoch)\n",
    "    \n",
    "    reruns_dimaml.append(maml_test_loss_history)\n",
    "    \n",
    "    print('Baseline')\n",
    "    maml.resample_parameters(initializers=maml.untrained_initializers, is_final=True)\n",
    "    base_model = deepcopy(maml.model)    \n",
    "    base_train_loss_history, base_test_loss_history = eval_model(base_model, wikitext2_train_loader, \n",
    "                                                                 part_wikitext2_test_ids, epochs=100, \n",
    "                                                                 device=device, mode='eval',\n",
    "                                                                 test_loss_interval=10*wikitext2_batches_in_epoch)\n",
    "    reruns_base.append(base_test_loss_history)\n",
    "    \n",
    "    print(\"Orthogonal\")\n",
    "    maml.resample_parameters(initializers=maml.untrained_initializers, is_final=True)\n",
    "    orthogonal_model = deepcopy(maml.model)\n",
    "    lstm_orthogonal(orthogonal_model.lstm1)\n",
    "    lstm_orthogonal(orthogonal_model.lstm2)\n",
    "    orthogonal_train_loss_history, orthogonal_test_loss_history = eval_model(orthogonal_model, wikitext2_train_loader,\n",
    "                                                                  part_wikitext2_test_ids, epochs=100, \n",
    "                                                                  device=device, mode='eval',\n",
    "                                                                  test_loss_interval=10*wikitext2_batches_in_epoch)\n",
    "    reruns_orthogonal.append(orthogonal_test_loss_history)\n",
    "    \n",
    "    print(\"Metainit\")\n",
    "    maml.resample_parameters(initializers=maml.untrained_initializers, is_final=True)\n",
    "    metainit_model = deepcopy(maml.model)\n",
    "    metainit(metainit_model, compute_loss, text.shape, wikitext2_voc_size)\n",
    "    metainit_train_loss_history, metainit_test_loss_history = eval_model(metainit_model, wikitext2_train_loader, \n",
    "                                                                         part_wikitext2_test_ids, epochs=100,\n",
    "                                                                         device=device, mode='eval', \n",
    "                                                                         test_loss_interval=10*wikitext2_batches_in_epoch)\n",
    "    reruns_metainit.append(metainit_test_loss_history)\n",
    "    \n",
    "reruns_base = np.array(reruns_base) * math.log2(math.e)\n",
    "reruns_dimaml = np.array(reruns_dimaml) * math.log2(math.e)\n",
    "reruns_metainit = np.array(reruns_metainit) * math.log2(math.e)\n",
    "reruns_orthogonal = np.array(reruns_orthogonal) * math.log2(math.e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline 10 epoch: \", reruns_base.mean(0)[1], reruns_base.std(0, ddof=1)[1])\n",
    "print(\"Baseline 50 epoch: \", reruns_base.mean(0)[5], reruns_base.std(0, ddof=1)[5])\n",
    "print(\"Baseline 100 epoch: \", reruns_base.mean(0)[10], reruns_base.std(0, ddof=1)[10])\n",
    "print()\n",
    "print(\"DIMAML 10 epoch: \", reruns_dimaml.mean(0)[1], reruns_dimaml.std(0, ddof=1)[1])\n",
    "print(\"DIMAML 50 epoch: \", reruns_dimaml.mean(0)[5], reruns_dimaml.std(0, ddof=1)[5])\n",
    "print(\"DIMAML 100 epoch: \", reruns_dimaml.mean(0)[10], reruns_dimaml.std(0, ddof=1)[10])\n",
    "print()\n",
    "print(\"MetaInit 10 epoch: \", fixed_reruns_metainit.mean(0)[1], fixed_reruns_metainit.std(0, ddof=1)[1])\n",
    "print(\"MetaInit 50 epoch: \", fixed_reruns_metainit.mean(0)[5], fixed_reruns_metainit.std(0, ddof=1)[5])\n",
    "print(\"MetaInit 100 epoch: \", fixed_reruns_metainit.mean(0)[10], fixed_reruns_metainit.std(0, ddof=1)[10])\n",
    "print()\n",
    "print(\"Orthogonal 10 epoch: \", reruns_orthogonal.mean(0)[1], reruns_orthogonal.std(0, ddof=1)[1])\n",
    "print(\"Orthogonal 50 epoch: \", reruns_orthogonal.mean(0)[5], reruns_orthogonal.std(0, ddof=1)[5])\n",
    "print(\"Orthogonal 100 epoch: \", reruns_orthogonal.mean(0)[10], reruns_orthogonal.std(0, ddof=1)[10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
