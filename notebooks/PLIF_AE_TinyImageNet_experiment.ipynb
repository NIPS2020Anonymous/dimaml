{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder model\n",
    "\n",
    "Training is on Tiny ImageNet. Evaluation is on CelebA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "import os, sys, time\n",
    "import warnings\n",
    "sys.path.insert(0, '..')\n",
    "import lib\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# For reproducibility\n",
    "import random\n",
    "seed = random.randint(0, 2 ** 32 - 1)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'AE'\n",
    "\n",
    "# Dataset \n",
    "data_dir = './data'\n",
    "train_batch_size = 128\n",
    "valid_batch_size = 256\n",
    "test_batch_size = 128\n",
    "num_workers = 3\n",
    "pin_memory = True\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# AE\n",
    "latent_dim = 64 \n",
    "loss_function = F.mse_loss\n",
    "num_quantiles = 100\n",
    "\n",
    "# MAML\n",
    "max_epochs = 3000\n",
    "batches_in_epoch = 200\n",
    "maml_epochs = 3\n",
    "maml_steps = batches_in_epoch * maml_epochs\n",
    "\n",
    "max_meta_parameters_grad_norm = 10.\n",
    "\n",
    "loss_kwargs={'reduction':'mean'}\n",
    "loss_interval = 50\n",
    "first_valid_step = 200\n",
    "\n",
    "assert (maml_steps - first_valid_step) % loss_interval == 0\n",
    "validation_steps = int((maml_steps - first_valid_step) / loss_interval + 1)\n",
    "\n",
    "# Optimizer\n",
    "optimizer_type='momentum'\n",
    "nesterov = False\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = None\n",
    "\n",
    "# Meta optimizer\n",
    "meta_learning_rate = 0.001\n",
    "meta_betas = (0.9, 0.997)\n",
    "meta_decay_interval = max_epochs\n",
    "\n",
    "checkpoint_steps = 5\n",
    "recovery_step = None\n",
    "\n",
    "kwargs = dict(\n",
    "    first_valid_step=first_valid_step,\n",
    "    valid_loss_interval=loss_interval, \n",
    "    loss_kwargs=loss_kwargs, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = f\"PLIF_{model_type}{latent_dim}_tiny_imagenet_{optimizer_type}_lr{learning_rate}\" + \\\n",
    "           f\"_meta_lr{meta_learning_rate}_steps{maml_steps}_interval{loss_interval}\" + \\\n",
    "           f\"_tr_bs{train_batch_size}_val_bs{valid_batch_size}_seed_{seed}\"\n",
    "        \n",
    "print(\"Experiment name: \", exp_name)\n",
    "\n",
    "logs_path = \"./logs/{}\".format(exp_name)\n",
    "assert recovery_step is not None or not os.path.exists(logs_path)\n",
    "# !rm -rf {logs_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNormalize(object):\n",
    "    def __init__(self, mean_image, std_image):\n",
    "        self.mean_image = mean_image\n",
    "        self.std_image = std_image\n",
    "        \n",
    "    def __call__(self, image):\n",
    "        normalized_image = (image - self.mean_image) / self.std_image\n",
    "        return normalized_image\n",
    "\n",
    "    \n",
    "class Flip(object):\n",
    "    def __call__(self, image):\n",
    "        if random.random() > 0.5:\n",
    "            return image.flip(-1)\n",
    "        else:\n",
    "            return image\n",
    "        \n",
    "        \n",
    "class CustomTensorDataset(torch.utils.data.Dataset):\n",
    "    \"\"\" TensorDataset with support of transforms \"\"\"\n",
    "    def __init__(self, *tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and valid data\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_dir = 'data/tiny-imagenet-200/'\n",
    "\n",
    "train_image_dataset =  datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms.ToTensor())\n",
    "train_images = torch.cat([train_image_dataset[i][0][None] for i in range(len(train_image_dataset))], dim=0)\n",
    "mean_image = train_images.mean(0)\n",
    "std_image = train_images.std(0)\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    Flip(),\n",
    "    PixelNormalize(mean_image, std_image),\n",
    "])\n",
    "\n",
    "eval_transforms = transforms.Compose([\n",
    "    PixelNormalize(mean_image, std_image),\n",
    "])\n",
    "\n",
    "train_dataset = CustomTensorDataset(train_images, transform=train_transforms)\n",
    "\n",
    "if batches_in_epoch * train_batch_size >= len(train_dataset):\n",
    "    warnings.warn(\"Your training process involves one entire epoch\")\n",
    "    \n",
    "valid_image_dataset =  datasets.ImageFolder(os.path.join(data_dir, 'val'), transforms.ToTensor())\n",
    "valid_images = torch.cat([valid_image_dataset[i][0][None] for i in range(len(valid_image_dataset))], dim=0)\n",
    "valid_dataset = CustomTensorDataset(valid_images, transform=eval_transforms)\n",
    "\n",
    "test_image_dataset =  datasets.ImageFolder(os.path.join(data_dir, 'test'), transforms.ToTensor())\n",
    "test_images = torch.cat([test_image_dataset[i][0][None] for i in range(len(test_image_dataset))], dim=0)\n",
    "test_dataset = CustomTensorDataset(test_images, transform=eval_transforms)\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=train_batch_size, shuffle=True,\n",
    "    num_workers=num_workers, pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, batch_size=valid_batch_size, shuffle=True,\n",
    "    num_workers=num_workers, pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=test_batch_size, shuffle=False, \n",
    "    num_workers=num_workers, pin_memory=pin_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_test_loss(model, loss_function, test_loader, device='cuda'):\n",
    "    model.eval()   \n",
    "    test_loss = 0.\n",
    "    for batch_test in test_loader:\n",
    "        if isinstance(batch_test, (list, tuple)):\n",
    "            x_test = batch_test[0].to(device)\n",
    "        elif isinstance(batch_test, torch.Tensor):\n",
    "            x_test = batch_test.to(device)\n",
    "        else:\n",
    "            raise Exception(\"Wrong batch\")\n",
    "        preds = model(x_test)\n",
    "        test_loss += loss_function(preds, x_test) * x_test.shape[0]\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    model.train()\n",
    "    return test_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if optimizer_type == 'sgd':\n",
    "    optimizer = lib.IngraphGradientDescent(learning_rate=learning_rate)\n",
    "elif optimizer_type == 'momentum':\n",
    "    optimizer = lib.IngraphMomentum(learning_rate=learning_rate, momentum=momentum, \n",
    "                                    weight_decay=weight_decay, nesterov=nesterov)\n",
    "elif optimizer_type == 'rmsprop':\n",
    "    optimizer = lib.IngraphRMSProp(learning_rate=learning_rate, beta=beta, epsilon=epsilon)\n",
    "elif optimizer_type == 'adam':\n",
    "    optimizer = lib.IngraphAdam(learning_rate=learning_rate, beta2=beta2, beta1=beta1, epsilon=epsilon)\n",
    "else: \n",
    "    raise NotImplemetedError(\"This optimizer is not implemeted\")\n",
    "\n",
    "model = lib.models.AE(latent_dim)\n",
    "maml = lib.PLIF_MAML(model, model_type, optimizer=optimizer, \n",
    "    checkpoint_steps=checkpoint_steps,\n",
    "    loss_function=loss_function,\n",
    "    num_quantiles=num_quantiles\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Staff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.parallel\n",
    "from concurrent.futures import Future, ThreadPoolExecutor, as_completed, TimeoutError\n",
    "GLOBAL_EXECUTOR = ThreadPoolExecutor(max_workers=16)\n",
    "\n",
    "def run_in_background(func: callable, *args, **kwargs) -> Future:\n",
    "    \"\"\" run func(*args, **kwargs) in background and return Future for its outputs \"\"\"\n",
    "    return GLOBAL_EXECUTOR.submit(func, *args, **kwargs)\n",
    "\n",
    "\n",
    "def samples_batches(dataloader, num_batches):\n",
    "    x_batches = []\n",
    "    for batch_i, x_batch in enumerate(dataloader):\n",
    "        if batch_i >= num_batches: break\n",
    "        x_batches.append(x_batch)\n",
    "    return x_batches\n",
    "\n",
    "\n",
    "def compute_maml_loss(maml, x_batches, y_batches, x_val_batches, y_val_batches, device):\n",
    "    with lib.training_mode(maml, is_train=True):\n",
    "        maml.resample_parameters()\n",
    "        updated_model, train_loss_history, valid_loss_history, *etc = \\\n",
    "            maml.forward(x_batches, y_batches, x_val_batches, y_val_batches, \n",
    "                         device=device, **kwargs)  \n",
    "        train_loss = torch.cat(train_loss_history[first_valid_step:]).mean()\n",
    "        valid_loss = torch.cat(valid_loss_history).mean() if len(valid_loss_history) > 0 else torch.zeros(1)\n",
    "    return train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerAE(lib.Trainer):\n",
    "    def train_on_batch(self, train_loader, valid_loader, prefix='train/', **kwargs):\n",
    "        \"\"\" Performs a single gradient update and reports metrics \"\"\"\n",
    "        x_batches = []\n",
    "        for _ in range(maml_epochs):\n",
    "            x_batches.extend(samples_batches(train_loader, batches_in_epoch))\n",
    "        x_val_batches = samples_batches(valid_loader, validation_steps)\n",
    "\n",
    "        self.meta_optimizer.zero_grad()\n",
    "        train_loss, valid_loss = compute_maml_loss(self.maml, x_batches, x_batches, \n",
    "                                                   x_val_batches, x_val_batches, self.device)\n",
    "        valid_loss.backward()\n",
    "        \n",
    "        global_grad_norm = nn.utils.clip_grad_norm_(self.maml.initializers.parameters(), \n",
    "                                                    max_meta_parameters_grad_norm)\n",
    "        self.writer.add_scalar(prefix + \"global_grad_norm\", global_grad_norm, self.total_steps)\n",
    "        \n",
    "        for i, param in enumerate(self.maml.initializers.parameters()):\n",
    "            if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
    "                print(\"Outer Nan or inf in grads\")\n",
    "            param.grad = torch.where(torch.isnan(param.grad), torch.zeros_like(param.grad), param.grad)\n",
    "            param.grad = torch.where(torch.isinf(param.grad), torch.zeros_like(param.grad), param.grad)\n",
    "            \n",
    "        self.meta_optimizer.step()\n",
    "        self.logs.append((self.total_steps, global_grad_norm))\n",
    "        \n",
    "        return self.record(train_loss=train_loss.item(),\n",
    "                           valid_loss=valid_loss.item(), prefix=prefix)\n",
    "    \n",
    "    def parallel_train_on_batch(self, train_loader, valid_loader, prefix='train/', **kwargs):\n",
    "        # generate training/validation batches for each device\n",
    "        replica_devices = [f'cuda:{i}' for i in range(torch.cuda.device_count())]\n",
    "        replicas = torch.nn.parallel.replicate(self.maml, devices=replica_devices, detach=False)\n",
    "\n",
    "        replica_inputs = []\n",
    "        for i, (replica, replica_device) in enumerate(zip(replicas, replica_devices)):\n",
    "            replica.untrained_initializers = lib.switch_initializers_device(replica.untrained_initializers, \n",
    "                                                                            replica_device)            \n",
    "            x_batches = []\n",
    "            for _ in range(maml_epochs):\n",
    "                x_batches.extend(samples_batches(train_loader, batches_in_epoch))\n",
    "            x_val_batches = samples_batches(valid_loader, validation_steps)\n",
    "\n",
    "            replica_inputs.append((replica, x_batches, x_batches,\n",
    "                                   x_val_batches, x_val_batches, replica_device))\n",
    "        \n",
    "        self.meta_optimizer.zero_grad()\n",
    "        replica_losses_futures = [run_in_background(compute_maml_loss, *replica_input)\n",
    "                                  for replica_input in replica_inputs]\n",
    "        \n",
    "        replica_losses = [future.result() for future in replica_losses_futures]\n",
    "        train_loss = sum(train_loss.item() for train_loss, _ in replica_losses) / len(replica_losses)\n",
    "        valid_loss = sum(valid_loss.to(self.device) for _, valid_loss in replica_losses) / len(replica_losses)\n",
    "        \n",
    "        valid_loss.backward()\n",
    "        \n",
    "        global_grad_norm = nn.utils.clip_grad_norm_(list(self.maml.initializers.parameters()), \n",
    "                                                    max_meta_parameters_grad_norm)\n",
    "        self.writer.add_scalar(prefix + \"global_grad_norm\", global_grad_norm, self.total_steps)\n",
    "        \n",
    "        for i, param in enumerate(self.maml.initializers.parameters()):\n",
    "            if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
    "                print(\"Outer Nan or inf in grads\")\n",
    "            param.grad = torch.where(torch.isnan(param.grad), torch.zeros_like(param.grad), param.grad)\n",
    "            param.grad = torch.where(torch.isinf(param.grad), torch.zeros_like(param.grad), param.grad)\n",
    "        \n",
    "        self.meta_optimizer.step()\n",
    "        self.logs.append((self.total_steps, global_grad_norm))\n",
    "        \n",
    "        return self.record(train_loss=train_loss,\n",
    "                           valid_loss=valid_loss.item(), prefix=prefix)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def plot_quantile_functions(self):\n",
    "        plt.figure(figsize=[20, 35])\n",
    "        i = 0\n",
    "        for name, (weight_quantile_function, bias_quantile_function) in self.maml.initializers.items():\n",
    "            wq_init, bq_init = self.maml.untrained_initializers[name]\n",
    "            i += 1\n",
    "            plt.subplot(6, 4, i)\n",
    "            xx = torch.linspace(0., 1., 1000).to(self.device)\n",
    "            yy = wq_init(xx)\n",
    "            plt.plot(lib.check_numpy(xx), lib.check_numpy(yy), '--')\n",
    "            yy = weight_quantile_function(xx)\n",
    "            plt.plot(lib.check_numpy(xx), lib.check_numpy(yy), c='g')\n",
    "            plt.xlim([0, 1])\n",
    "            plt.title(name + '_weight', fontsize=14)\n",
    "            plt.yticks(fontsize=12)\n",
    "            plt.xticks(fontsize=12)\n",
    "        \n",
    "            if i in [9, 10, 11, 12]:\n",
    "                plt.xlabel(\"U(0,1) samples\", fontsize=14)\n",
    "                    \n",
    "            if bias_quantile_function is not None:\n",
    "                i += 1\n",
    "                plt.subplot(6, 4, i)\n",
    "                if i == 12:\n",
    "                    plt.plot(lib.check_numpy(xx), lib.check_numpy(yy), '--', label='Kaiming')\n",
    "                    yy = bias_quantile_function(xx)\n",
    "                    plt.plot(lib.check_numpy(xx), lib.check_numpy(yy), c='g', label='DIMAML')\n",
    "                    leg = plt.legend(loc=4, fontsize=15, frameon=False)\n",
    "                    for line in leg.get_lines():\n",
    "                        line.set_linewidth(1.6)\n",
    "                else:\n",
    "                    yy = bq_init(xx)\n",
    "                    plt.plot(lib.check_numpy(xx), lib.check_numpy(yy), '--',)\n",
    "                    yy = bias_quantile_function(xx)\n",
    "                    plt.plot(lib.check_numpy(xx), lib.check_numpy(yy), c='g')\n",
    "                plt.xlim([0, 1])\n",
    "                plt.title(name + '_bias', fontsize=14)\n",
    "                plt.yticks(fontsize=12)\n",
    "                plt.xticks(fontsize=12)\n",
    "            \n",
    "                if i in [9, 10, 11, 12]:\n",
    "                    plt.xlabel(\"U(0,1) samples\", fontsize=14)\n",
    "        plt.show()\n",
    "        \n",
    "    def evaluate_metrics(self, train_loader, test_loader, prefix='val/', **kwargs):\n",
    "        \"\"\" Predicts and evaluates metrics over the entire dataset \"\"\"\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print('Baseline')\n",
    "        self.maml.resample_parameters(initializers=self.maml.untrained_initializers, is_final=True)\n",
    "        base_model = deepcopy(self.maml.model)    \n",
    "        base_train_loss_history, base_test_loss_history = eval_model(base_model, train_loader, test_loader,\n",
    "                                                                     device=self.device, **kwargs)\n",
    "        print('Ours')\n",
    "        self.maml.resample_parameters(is_final=True)\n",
    "        maml_model = deepcopy(self.maml.model)\n",
    "        maml_train_loss_history, maml_test_loss_history = eval_model(maml_model, train_loader, test_loader, \n",
    "                                                                     device=self.device,  **kwargs)\n",
    "        draw_plots(base_train_loss_history, base_test_loss_history, \n",
    "                   maml_train_loss_history, maml_test_loss_history)\n",
    "        \n",
    "        self.writer.add_scalar(prefix + \"test_AUC\", sum(maml_test_loss_history), self.total_steps)\n",
    "        self.writer.add_scalar(prefix + \"test_loss\", maml_test_loss_history[-1], self.total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Generate Train Batch #\n",
    "########################\n",
    "            \n",
    "def generate_train_batches(train_loader, batches_in_epoch=150):\n",
    "    x_batches = []\n",
    "    for batch_i, x_batch in enumerate(train_loader):\n",
    "        if batch_i >= batches_in_epoch: break\n",
    "        x_batches.append(x_batch)\n",
    "\n",
    "    assert len(x_batches) == batches_in_epoch\n",
    "    local_x = torch.cat(x_batches, dim=0)\n",
    "    return DataLoader(local_x, batch_size=train_batch_size, shuffle=True, \n",
    "                      num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "##################\n",
    "# Eval functions #\n",
    "##################\n",
    "\n",
    "\n",
    "def eval_model(model, train_loader, test_loader, batches_in_epoch=150, \n",
    "               epochs=3, test_loss_interval=50, device='cuda', **kwargs):\n",
    "    if optimizer_type == 'sgd':\n",
    "        optimizer = torch.optim.SGD(maml.get_parameters(model), lr=learning_rate)\n",
    "    elif optimizer_type == 'momentum':\n",
    "        torch_weight_decay = 0.0 if weight_decay is None else weight_decay\n",
    "        optimizer = torch.optim.SGD(maml.get_parameters(model), lr=learning_rate, \n",
    "                                    momentum=momentum, weight_decay=torch_weight_decay, nesterov=nesterov)\n",
    "    elif optimizer_type == 'rmsprop':\n",
    "        optimizer = torch.optim.RMSProp(maml.get_parameters(model), lr=learning_rate, beta=beta, eps=epsilon)\n",
    "    elif optimizer_type == 'adam':\n",
    "        optimizer = torch.optim.Adam(maml.get_parameters(model), lr=learning_rate, \n",
    "                                     betas=(beta1, beta2), epsilon=epsilon)\n",
    "    else: \n",
    "        raise NotImplemetedError(\"{} optimizer is not implemeted\".format(optimizer_type))\n",
    "        \n",
    "    # Train loop\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "    \n",
    "    training_mode = model.training\n",
    "    total_iters = 0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        for x_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x_batch = x_batch.to(device)\n",
    "            preds = model(x_batch)\n",
    "            loss = loss_function(preds, x_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_history.append(loss.item())\n",
    "            \n",
    "            if (total_iters == 0) or (total_iters + 1) % test_loss_interval == 0: \n",
    "                model.eval()\n",
    "                test_loss = compute_test_loss(model, loss_function, test_loader, device=device)\n",
    "                print(\"Epoch {} | Total Iteration {} | Loss {}\".format(epoch, total_iters+1, test_loss))\n",
    "                test_loss_history.append(test_loss)\n",
    "                model.train()\n",
    "            \n",
    "            total_iters += 1    \n",
    "    model.train(training_mode)\n",
    "    return train_loss_history, test_loss_history\n",
    "\n",
    "\n",
    "def draw_plots(base_train_loss, base_test_loss, maml_train_loss, maml_test_loss):\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(moving_average(base_train_loss, span=10), label='Baseline')\n",
    "    plt.plot(moving_average(maml_train_loss, span=10), c='g', label='Ours')\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.title(\"Train loss\", fontsize=14)\n",
    "    plt.xlabel(\"Steps\", fontsize=14)\n",
    "    plt.ylabel(\"MSE\", fontsize=14)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(base_test_loss, label='Baseline')\n",
    "    plt.plot(maml_test_loss, c='g', label='Ours')\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.title(\"Test loss\", fontsize=14)\n",
    "    plt.xlabel(\"Steps\", fontsize=14)\n",
    "    plt.ylabel(\"MSE\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from pandas import DataFrame\n",
    "\n",
    "moving_average = lambda x, **kw: DataFrame({'x':np.asarray(x)}).x.ewm(**kw).mean().values\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "trainer = TrainerAE(maml, decay_interval=meta_decay_interval, meta_lr=meta_learning_rate, \n",
    "                    meta_betas=meta_betas, exp_name=exp_name, recovery_step=recovery_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.free_memory()\n",
    "t0 = time.time()\n",
    "\n",
    "while trainer.total_steps <= max_epochs:\n",
    "    local_train_loader = generate_train_batches(train_loader, batches_in_epoch)\n",
    "    \n",
    "    with lib.activate_context_batchnorm(maml.model):\n",
    "        metrics = trainer.train_on_batch(\n",
    "            local_train_loader, valid_loader,\n",
    "            first_valid_step=first_valid_step,\n",
    "            valid_loss_interval=loss_interval, \n",
    "            loss_kwargs=loss_kwargs, \n",
    "        )\n",
    "    train_loss = metrics['train_loss']\n",
    "    train_loss_history.append(train_loss)\n",
    "    \n",
    "    valid_loss = metrics['valid_loss']\n",
    "    valid_loss_history.append(valid_loss)\n",
    "\n",
    "    if trainer.total_steps % 10 == 0:\n",
    "        clear_output(True)\n",
    "        print(\"Step: %d | Time: %f | Train Loss %.5f | Valid loss %.5f\" \n",
    "              % (trainer.total_steps, time.time()-t0, train_loss, valid_loss))\n",
    "        plt.figure(figsize=[16, 5])\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.title('Train Loss over time')\n",
    "        plt.plot(moving_average(train_loss_history, span=50))\n",
    "        plt.scatter(range(len(train_loss_history)), train_loss_history, alpha=0.1)\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.title('Valid Loss over time')\n",
    "        plt.plot(moving_average(valid_loss_history, span=50))\n",
    "        plt.scatter(range(len(valid_loss_history)), valid_loss_history, alpha=0.1)\n",
    "        plt.show()\n",
    "        trainer.evaluate_metrics(local_train_loader, test_loader, epochs=maml_epochs,\n",
    "                                 test_loss_interval=loss_interval)\n",
    "        trainer.plot_quantile_functions()\n",
    "        t0 = time.time()\n",
    "        \n",
    "    if trainer.total_steps % 100 == 0:\n",
    "        trainer.save_model()\n",
    "        \n",
    "    trainer.total_steps += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.plot_quantile_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_quotient(loss, params, eps=1e-5): \n",
    "    grad = torch.autograd.grad(loss, params, retain_graph=True, create_graph=True)\n",
    "    prod = torch.autograd.grad(sum([(g**2).sum() / 2 for g in grad]),\n",
    "                               params, retain_graph=True, create_graph=True)\n",
    "    out = sum([((g - p) / (g + eps * (2*(g >= 0).float() - 1).detach()) - 1).abs().sum() \n",
    "               for g, p in zip(grad, prod)])\n",
    "    return out / sum([p.data.nelement() for p in params])\n",
    "\n",
    "\n",
    "def metainit(model, criterion, x_size, lr=0.1, momentum=0.9, steps=200, eps=1e-5):\n",
    "    model.eval()\n",
    "    params = [p for p in model.parameters() \n",
    "              if p.requires_grad and len(p.size()) >= 2]\n",
    "    memory = [0] * len(params)\n",
    "    for i in range(steps):\n",
    "        input = torch.Tensor(*x_size).normal_(0, 1).cuda()\n",
    "        loss = criterion(model(input), input)\n",
    "        gq = gradient_quotient(loss, list(model.parameters()), eps)\n",
    "        \n",
    "        grad = torch.autograd.grad(gq, params)\n",
    "        for j, (p, g_all) in enumerate(zip(params, grad)):\n",
    "            norm = p.data.norm().item()\n",
    "            g = torch.sign((p.data * g_all).sum() / norm) \n",
    "            memory[j] = momentum * memory[j] - lr * g.item() \n",
    "            new_norm = norm + memory[j]\n",
    "            p.data.mul_(new_norm / (norm + eps))\n",
    "        print(\"%d/GQ = %.2f\" % (i, gq.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genOrthgonal(dim):\n",
    "    a = torch.zeros((dim, dim)).normal_(0, 1)\n",
    "    q, r = torch.qr(a)\n",
    "    d = torch.diag(r, 0).sign()\n",
    "    diag_size = d.size(0)\n",
    "    d_exp = d.view(1, diag_size).expand(diag_size, diag_size)\n",
    "    q.mul_(d_exp)\n",
    "    return q\n",
    "\n",
    "def makeDeltaOrthogonal(weights, gain):\n",
    "    rows = weights.size(0)\n",
    "    cols = weights.size(1)\n",
    "    if rows < cols:\n",
    "        print(\"In_filters should not be greater than out_filters.\")\n",
    "    weights.data.fill_(0)\n",
    "    dim = max(rows, cols)\n",
    "    q = genOrthgonal(dim)\n",
    "    mid1 = weights.size(2) // 2\n",
    "    mid2 = weights.size(3) // 2\n",
    "    with torch.no_grad():\n",
    "        weights[:, :, mid1, mid2] = q[:weights.size(0), :weights.size(1)]\n",
    "        weights.mul_(gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval CelebA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download this file into dataset_directory and unzip it:\n",
    "#  https://drive.google.com/open?id=0B7EVK8r0v71pZjFTYXZWM3FlRnM\n",
    "# 2. Put the `img_align_celeba` directory into the `celeba` directory!\n",
    "# 3. Dataset directory structure should look like this (required by ImageFolder from torchvision):\n",
    "#  +- `dataset_directory`\n",
    "#     +- celeba\n",
    "#        +- train\n",
    "#           +- images\n",
    "#              +- 000001.jpg\n",
    "#              +- 000002.jpg\n",
    "#              +- ...\n",
    "#        +- val\n",
    "#           +- images\n",
    "#              +- 000001.jpg\n",
    "#              +- 000002.jpg\n",
    "#              +- ...\n",
    "#        +- test\n",
    "#           +- images\n",
    "#              +- 000001.jpg\n",
    "#              +- 000002.jpg\n",
    "#              +- ...\n",
    "\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "celeba_data_dir = 'data/celeba/'\n",
    "data = pd.read_csv(os.path.join(celeba_data_dir, 'list_eval_partition.csv'))\n",
    "\n",
    "# Uncomment if you want to copy data again\n",
    "# for partition in ['train', 'val', 'test']:\n",
    "#     image_path = os.path.join(celeba_data_dir, partition)\n",
    "#     !rm -rf {image_path}\n",
    "\n",
    "try:\n",
    "    for partition in ['train', 'val', 'test']:\n",
    "        os.makedirs(os.path.join(celeba_data_dir, partition))\n",
    "        os.makedirs(os.path.join(celeba_data_dir, partition, 'images'))\n",
    "\n",
    "    for i in data.index:\n",
    "        partition = data.loc[i].partition\n",
    "        src_path = os.path.join(celeba_data_dir, 'img_align_celeba/img_align_celeba', data.loc[i].image_id)\n",
    "        if partition == 0:\n",
    "            shutil.copyfile(src_path, os.path.join(celeba_data_dir, 'train', 'images', data.loc[i].image_id))\n",
    "        elif partition == 1:\n",
    "            shutil.copyfile(src_path, os.path.join(celeba_data_dir, 'val', 'images', data.loc[i].image_id))\n",
    "        elif partition == 2:\n",
    "            shutil.copyfile(src_path, os.path.join(celeba_data_dir, 'test', 'images', data.loc[i].image_id))\n",
    "            \n",
    "except FileExistsError:\n",
    "    print('\\'train\\', \\'val\\', \\'test\\' already exist. Probably, you do not want to copy data again')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeba_transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "celeba_train_dataset = datasets.ImageFolder(celeba_data_dir+'train', transform=celeba_transforms)\n",
    "celeba_train_images = torch.cat([celeba_train_dataset[i][0][None] for i in range(len(celeba_train_dataset))])\n",
    "\n",
    "celeba_mean_image = celeba_train_images.mean(0)\n",
    "celeba_std_image = celeba_train_images.std(0)\n",
    "\n",
    "celeba_train_images = (celeba_train_images - celeba_mean_image) / celeba_std_image\n",
    "\n",
    "celeba_test_dataset = datasets.ImageFolder(celeba_data_dir+'test', celeba_transforms)\n",
    "celeba_test_images = torch.cat([celeba_test_dataset[i][0][None] for i in range(len(celeba_test_dataset))])\n",
    "celeba_test_images = (celeba_test_images - celeba_mean_image) / celeba_std_image\n",
    "\n",
    "celeba_train_loader = torch.utils.data.DataLoader(celeba_train_images, batch_size=train_batch_size, shuffle=True,\n",
    "                                                  pin_memory=pin_memory, num_workers=num_workers)\n",
    "celeba_test_loader = torch.utils.data.DataLoader(celeba_test_images, batch_size=test_batch_size, \n",
    "                                                  pin_memory=pin_memory, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Evaluate models #\n",
    "###################\n",
    "\n",
    "num_reruns=10\n",
    "celeba_batches_in_epoch = len(celeba_train_loader) # 1272 - full epoch\n",
    "\n",
    "celeba_base_runs_10 = []\n",
    "celeba_base_runs_50 = []\n",
    "celeba_base_runs_100 = []\n",
    "\n",
    "celeba_maml_runs_10 = []\n",
    "celeba_maml_runs_50 = []\n",
    "celeba_maml_runs_100 = []\n",
    "\n",
    "celeba_deltaorthogonal_runs_10 = []\n",
    "celeba_deltaorthogonal_runs_50 = []\n",
    "celeba_deltaorthogonal_runs_100 = []\n",
    "\n",
    "celeba_metainit_runs_10 = []\n",
    "celeba_metainit_runs_50 = []\n",
    "celeba_metainit_runs_100 = []\n",
    "\n",
    "for _ in range(num_reruns):\n",
    "    print(\"Baseline\")\n",
    "    maml.resample_parameters(initializers=maml.untrained_initializers, is_final=True)\n",
    "    base_model = deepcopy(maml.model)    \n",
    "    base_train_loss_history, base_test_loss_history = \\\n",
    "        eval_model(base_model, celeba_train_loader, celeba_test_loader, \n",
    "                   batches_in_epoch=celeba_batches_in_epoch, epochs=100, \n",
    "                   test_loss_interval=10*celeba_batches_in_epoch, device=device)\n",
    "\n",
    "    print(\"Ours\")\n",
    "    maml.resample_parameters(is_final=True)\n",
    "    maml_model = deepcopy(maml.model)\n",
    "    maml_train_loss_history, maml_test_loss_history = \\\n",
    "        eval_model(maml_model, celeba_train_loader, celeba_test_loader, \n",
    "                   batches_in_epoch=celeba_batches_in_epoch, epochs=100, \n",
    "                   test_loss_interval=10*celeba_batches_in_epoch, device=device)\n",
    "    \n",
    "    celeba_base_runs_10.append(base_test_loss_history[1])\n",
    "    celeba_base_runs_50.append(base_test_loss_history[5])\n",
    "    celeba_base_runs_100.append(base_test_loss_history[10])\n",
    "    \n",
    "    celeba_maml_runs_10.append(maml_test_loss_history[1])\n",
    "    celeba_maml_runs_50.append(maml_test_loss_history[5])\n",
    "    celeba_maml_runs_100.append(maml_test_loss_history[10])\n",
    "    \n",
    "    print(\"MetaInit\")\n",
    "    batch_x = next(iter(celeba_train_loader))\n",
    "    maml.resample_parameters(initializers=maml.untrained_initializers, is_final=True)\n",
    "    metainit_model = deepcopy(maml.model)\n",
    "    metainit(metainit_model, loss_function, batch_x.shape, steps=200)\n",
    "\n",
    "    metainit_train_loss_history, metainit_test_loss_history = \\\n",
    "        eval_model(metainit_model, celeba_train_loader, celeba_test_loader, \n",
    "                   batches_in_epoch=celeba_batches_in_epoch, epochs=100, \n",
    "                   test_loss_interval=10*celeba_batches_in_epoch, device=device)\n",
    "    \n",
    "    print(\"Delta Orthogonal\")\n",
    "    maml.resample_parameters(initializers=maml.untrained_initializers, is_final=True)\n",
    "    deltaorthogonal_model = deepcopy(maml.model)\n",
    "    for param in deltaorthogonal_model.parameters():\n",
    "        if len(param.size()) >= 4:\n",
    "            makeDeltaOrthogonal(param, nn.init.calculate_gain('relu'))\n",
    "    \n",
    "    deltaorthogonal_train_loss_history, deltaorthogonal_test_loss_history = \\\n",
    "        eval_model(deltaorthogonal_model, celeba_train_loader, celeba_test_loader, \n",
    "                   batches_in_epoch=celeba_batches_in_epoch, epochs=100, \n",
    "                   test_loss_interval=10*celeba_batches_in_epoch, device=device)\n",
    "    \n",
    "    kaiming_train_loss_history, kaiming_test_loss_history = \\\n",
    "        eval_model(kaiming_model, celeba_train_loader, celeba_test_loader, \n",
    "                   batches_in_epoch=celeba_batches_in_epoch, epochs=100, \n",
    "                   test_loss_interval=10*celeba_batches_in_epoch, device=device)\n",
    "    \n",
    "    celeba_deltaorthogonal_runs_10.append(deltaorthogonal_test_loss_history[1])\n",
    "    celeba_deltaorthogonal_runs_50.append(deltaorthogonal_test_loss_history[5])\n",
    "    celeba_deltaorthogonal_runs_100.append(deltaorthogonal_test_loss_history[10])\n",
    "    \n",
    "    celeba_metainit_runs_10.append(metainit_test_loss_history[1])\n",
    "    celeba_metainit_runs_50.append(metainit_test_loss_history[5])\n",
    "    celeba_metainit_runs_100.append(metainit_test_loss_history[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline 10 epoch: \", np.mean(celeba_base_runs_10), np.std(celeba_base_runs_10, ddof=1))\n",
    "print(\"Baseline 50 epoch: \", np.mean(celeba_base_runs_50), np.std(celeba_base_runs_50, ddof=1))\n",
    "print(\"Baseline 100 epoch: \", np.mean(celeba_base_runs_100), np.std(celeba_base_runs_100, ddof=1))\n",
    "print()\n",
    "print(\"DIMAML 10 epoch: \", np.mean(celeba_maml_runs_10), np.std(celeba_maml_runs_10, ddof=1))\n",
    "print(\"DIMAML 50 epoch: \", np.mean(celeba_maml_runs_50), np.std(celeba_maml_runs_50, ddof=1))\n",
    "print(\"DIMAML 100 epoch: \", np.mean(celeba_maml_runs_100), np.std(celeba_maml_runs_100, ddof=1))\n",
    "\n",
    "print(\"MetaInit 10 epoch: \", np.mean(celeba_metainit_runs_10), np.std(celeba_metainit_runs_10, ddof=1))\n",
    "print(\"MetaInit 50 epoch: \", np.mean(celeba_metainit_runs_50), np.std(celeba_metainit_runs_50, ddof=1))\n",
    "print(\"MetaInit 100 epoch: \", np.mean(celeba_metainit_runs_100), np.std(celeba_metainit_runs_100, ddof=1))\n",
    "print()\n",
    "print(\"DeltaOrthogonal 10 epoch: \", np.mean(celeba_deltaorthogonal_runs_10), np.std(celeba_deltaorthogonal_runs_10, ddof=1))\n",
    "print(\"DeltaOrthogonal 50 epoch: \", np.mean(celeba_deltaorthogonal_runs_50), np.std(celeba_deltaorthogonal_runs_50, ddof=1))\n",
    "print(\"DeltaOrthogonal 100 epoch: \", np.mean(celeba_deltaorthogonal_runs_100), np.std(celeba_deltaorthogonal_runs_100, ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AnimeFaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animefaces_transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "animefaces_dataset = datasets.ImageFolder('data/AnimeFaces/', transform=animefaces_transforms)\n",
    "animefaces_images = torch.cat([animefaces_dataset[i][0][None] for i in range(len(animefaces_dataset))])\n",
    "animefaces_train_images = animefaces_images[:19000]\n",
    "\n",
    "animefaces_mean_image = animefaces_train_images.mean(0)\n",
    "animefaces_std_image = animefaces_train_images.std(0)\n",
    "\n",
    "animefaces_train_images = (animefaces_train_images - animefaces_mean_image) / animefaces_std_image\n",
    "animefaces_train_dataset = CustomTensorDataset(animefaces_train_images, transform=Flip())\n",
    "\n",
    "animefaces_test_images = animefaces_images[19000:]\n",
    "animefaces_test_images = (animefaces_test_images - animefaces_mean_image) / animefaces_std_image\n",
    "\n",
    "animefaces_train_loader = torch.utils.data.DataLoader(animefaces_train_dataset, batch_size=train_batch_size, shuffle=True,\n",
    "                                                  pin_memory=pin_memory, num_workers=num_workers)\n",
    "animefaces_test_loader = torch.utils.data.DataLoader(animefaces_test_images, batch_size=test_batch_size, \n",
    "                                                  pin_memory=pin_memory, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Evaluate models #\n",
    "###################\n",
    "\n",
    "num_reruns=10\n",
    "animefaces_batches_in_epoch = len(animefaces_train_loader) # 1272 - full epoch\n",
    "\n",
    "animefaces_base_runs_10 = []\n",
    "animefaces_base_runs_50 = []\n",
    "animefaces_base_runs_100 = []\n",
    "\n",
    "animefaces_maml_runs_10 = []\n",
    "animefaces_maml_runs_50 = []\n",
    "animefaces_maml_runs_100 = []\n",
    "\n",
    "animefaces_deltaorthogonal_runs_10 = []\n",
    "animefaces_deltaorthogonal_runs_50 = []\n",
    "animefaces_deltaorthogonal_runs_100 = []\n",
    "\n",
    "animefaces_metainit_runs_10 = []\n",
    "animefaces_metainit_runs_50 = []\n",
    "animefaces_metainit_runs_100 = []\n",
    "\n",
    "for _ in range(num_reruns):\n",
    "    print(\"Baseline\")\n",
    "    maml.resample_parameters(initializers=maml.untrained_initializers, is_final=True)\n",
    "    base_model = deepcopy(maml.model)    \n",
    "    base_train_loss_history, base_test_loss_history = \\\n",
    "        eval_model(base_model, animefaces_train_loader, animefaces_test_loader, \n",
    "                   batches_in_epoch=animefaces_batches_in_epoch, epochs=100, \n",
    "                   test_loss_interval=10*animefaces_batches_in_epoch, device=device)\n",
    "\n",
    "    print(\"Ours\")\n",
    "    maml.resample_parameters(is_final=True)\n",
    "    maml_model = deepcopy(maml.model)\n",
    "    maml_train_loss_history, maml_test_loss_history = \\\n",
    "        eval_model(maml_model, animefaces_train_loader, animefaces_test_loader, \n",
    "                   batches_in_epoch=animefaces_batches_in_epoch, epochs=100, \n",
    "                   test_loss_interval=10*animefaces_batches_in_epoch, device=device)\n",
    "    \n",
    "    print(\"MetaInit\")\n",
    "    batch_x = next(iter(animefaces_train_loader))\n",
    "    maml.resample_parameters(initializers=maml.untrained_initializers, is_final=True)\n",
    "    metainit_model = deepcopy(maml.model)\n",
    "    metainit(metainit_model, loss_function, batch_x.shape, steps=200)\n",
    "\n",
    "    metainit_train_loss_history, metainit_test_loss_history = \\\n",
    "        eval_model(metainit_model, animefaces_train_loader, animefaces_test_loader, \n",
    "                   batches_in_epoch=animefaces_batches_in_epoch, epochs=100, \n",
    "                   test_loss_interval=10*animefaces_batches_in_epoch, device=device)\n",
    "    \n",
    "    print(\"Delta Orthogonal\")\n",
    "    maml.resample_parameters(initializers=maml.untrained_initializers, is_final=True)\n",
    "    deltaorthogonal_model = deepcopy(maml.model)\n",
    "    for param in deltaorthogonal_model.parameters():\n",
    "        if len(param.size()) >= 4:\n",
    "            makeDeltaOrthogonal(param, nn.init.calculate_gain('relu'))\n",
    "    \n",
    "    deltaorthogonal_train_loss_history, deltaorthogonal_test_loss_history = \\\n",
    "        eval_model(deltaorthogonal_model, animefaces_train_loader, animefaces_test_loader, \n",
    "                   batches_in_epoch=animefaces_batches_in_epoch, epochs=100, \n",
    "                   test_loss_interval=10*animefaces_batches_in_epoch, device=device)\n",
    "    \n",
    "    animefaces_base_runs_10.append(base_test_loss_history[1])\n",
    "    animefaces_base_runs_50.append(base_test_loss_history[5])\n",
    "    animefaces_base_runs_100.append(base_test_loss_history[10])\n",
    "    \n",
    "    animefaces_maml_runs_10.append(maml_test_loss_history[1])\n",
    "    animefaces_maml_runs_50.append(maml_test_loss_history[5])\n",
    "    animefaces_maml_runs_100.append(maml_test_loss_history[10])\n",
    "    \n",
    "    animefaces_deltaorthogonal_runs_10.append(deltaorthogonal_test_loss_history[1])\n",
    "    animefaces_deltaorthogonal_runs_50.append(deltaorthogonal_test_loss_history[5])\n",
    "    animefaces_deltaorthogonal_runs_100.append(deltaorthogonal_test_loss_history[10])\n",
    "    \n",
    "    animefaces_metainit_runs_10.append(metainit_test_loss_history[1])\n",
    "    animefaces_metainit_runs_50.append(metainit_test_loss_history[5])\n",
    "    animefaces_metainit_runs_100.append(metainit_test_loss_history[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline 10 epoch: \", np.mean(animefaces_base_runs_10), np.std(animefaces_base_runs_10, ddof=1))\n",
    "print(\"Baseline 50 epoch: \", np.mean(animefaces_base_runs_50), np.std(animefaces_base_runs_50, ddof=1))\n",
    "print(\"Baseline 100 epoch: \", np.mean(animefaces_base_runs_100), np.std(animefaces_base_runs_100, ddof=1))\n",
    "print()\n",
    "print(\"DIMAML 10 epoch: \", np.mean(animefaces_maml_runs_10), np.std(animefaces_maml_runs_10, ddof=1))\n",
    "print(\"DIMAML 50 epoch: \", np.mean(animefaces_maml_runs_50), np.std(animefaces_maml_runs_50, ddof=1))\n",
    "print(\"DIMAML 100 epoch: \", np.mean(animefaces_maml_runs_100), np.std(animefaces_maml_runs_100, ddof=1))\n",
    "print()\n",
    "print(\"MetaInit 10 epoch: \", np.mean(animefaces_metainit_runs_10), np.std(animefaces_metainit_runs_10, ddof=1))\n",
    "print(\"MetaInit 50 epoch: \", np.mean(animefaces_metainit_runs_50), np.std(animefaces_metainit_runs_50, ddof=1))\n",
    "print(\"MetaInit 100 epoch: \", np.mean(animefaces_metainit_runs_100), np.std(animefaces_metainit_runs_100, ddof=1))\n",
    "print()\n",
    "print(\"DeltaOrthogonal 10 epoch: \", np.mean(animefaces_deltaorthogonal_runs_10), np.std(animefaces_deltaorthogonal_runs_10, ddof=1))\n",
    "print(\"DeltaOrthogonal 50 epoch: \", np.mean(animefaces_deltaorthogonal_runs_50), np.std(animefaces_deltaorthogonal_runs_50, ddof=1))\n",
    "print(\"DeltaOrthogonal 100 epoch: \", np.mean(animefaces_deltaorthogonal_runs_100), np.std(animefaces_deltaorthogonal_runs_100, ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permuted AnimeFaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation = torch.load('nips_animefaces_permutation.pt')#torch.randperm(64*64)\n",
    "permuted_animefaces_train_images = []\n",
    "for image in animefaces_train_images:\n",
    "    for i in range(3):\n",
    "        image[i] = image[i].view(-1)[permutation].view(64, 64) # view !!!!!!!!!!\n",
    "    permuted_animefaces_train_images.append(image[None])\n",
    "permuted_animefaces_train_images = torch.cat(permuted_animefaces_train_images, dim=0)\n",
    "\n",
    "permuted_animefaces_test_images = []\n",
    "for image in animefaces_test_images:\n",
    "    for i in range(3):\n",
    "        image[i] = image[i].view(-1)[permutation].view(64, 64)  # view !!!!!!!!!!\n",
    "    permuted_animefaces_test_images.append(image[None])\n",
    "permuted_animefaces_test_images = torch.cat(permuted_animefaces_test_images, dim=0)\n",
    "\n",
    "\n",
    "permuted_animefaces_train_dataset = CustomTensorDataset(permuted_animefaces_train_images, transform=Flip())\n",
    "\n",
    "permuted_animefaces_train_loader = torch.utils.data.DataLoader(permuted_animefaces_train_dataset, \n",
    "                                                               batch_size=train_batch_size, shuffle=True,\n",
    "                                                               pin_memory=pin_memory, num_workers=num_workers)\n",
    "permuted_animefaces_test_loader = torch.utils.data.DataLoader(permuted_animefaces_test_images, \n",
    "                                                              batch_size=test_batch_size, \n",
    "                                                              pin_memory=pin_memory, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Evaluate models #\n",
    "###################\n",
    "\n",
    "num_reruns=10\n",
    "animefaces_batches_in_epoch = len(animefaces_train_loader) # 1272 - full epoch\n",
    "\n",
    "permuted_animefaces_base_runs_10 = []\n",
    "permuted_animefaces_base_runs_50 = []\n",
    "permuted_animefaces_base_runs_100 = []\n",
    "\n",
    "permuted_animefaces_maml_runs_10 = []\n",
    "permuted_animefaces_maml_runs_50 = []\n",
    "permuted_animefaces_maml_runs_100 = []\n",
    "\n",
    "permuted_animefaces_deltaorthogonal_runs_10 = []\n",
    "permuted_animefaces_deltaorthogonal_runs_50 = []\n",
    "permuted_animefaces_deltaorthogonal_runs_100 = []\n",
    "\n",
    "permuted_animefaces_metainit_runs_10 = []\n",
    "permuted_animefaces_metainit_runs_50 = []\n",
    "permuted_animefaces_metainit_runs_100 = []\n",
    "\n",
    "for _ in range(num_reruns):    \n",
    "    print(\"MetaInit\")\n",
    "    batch_x = next(iter(animefaces_train_loader))\n",
    "    maml.resample_parameters(initializers=maml.untrained_initializers, is_final=True)\n",
    "    metainit_model = deepcopy(maml.model)\n",
    "    metainit(metainit_model, loss_function, batch_x.shape, steps=200)\n",
    "\n",
    "    metainit_train_loss_history, metainit_test_loss_history = \\\n",
    "        eval_model(metainit_model, permuted_animefaces_train_loader, permuted_animefaces_test_loader, \n",
    "                   batches_in_epoch=animefaces_batches_in_epoch, epochs=100, \n",
    "                   test_loss_interval=10*animefaces_batches_in_epoch, device=device)\n",
    "    \n",
    "    print(\"Delta Orthogonal\")\n",
    "    maml.resample_parameters(initializers=maml.untrained_initializers, is_final=True)\n",
    "    deltaorthogonal_model = deepcopy(maml.model)\n",
    "    for param in deltaorthogonal_model.parameters():\n",
    "        if len(param.size()) >= 4:\n",
    "            makeDeltaOrthogonal(param, nn.init.calculate_gain('relu'))\n",
    "    \n",
    "    deltaorthogonal_train_loss_history, deltaorthogonal_test_loss_history = \\\n",
    "        eval_model(deltaorthogonal_model, permuted_animefaces_train_loader, permuted_animefaces_test_loader, \n",
    "                   batches_in_epoch=animefaces_batches_in_epoch, epochs=100, \n",
    "                   test_loss_interval=10*animefaces_batches_in_epoch, device=device)\n",
    "    \n",
    "    print(\"Baseline\")\n",
    "    maml.resample_parameters(initializers=maml.untrained_initializers, is_final=True)\n",
    "    base_model = deepcopy(maml.model)    \n",
    "    base_train_loss_history, base_test_loss_history = \\\n",
    "        eval_model(base_model, permuted_animefaces_train_loader, permuted_animefaces_test_loader, \n",
    "                   batches_in_epoch=animefaces_batches_in_epoch, epochs=100, \n",
    "                   test_loss_interval=10*animefaces_batches_in_epoch, device=device)\n",
    "\n",
    "    print(\"DIMAML\")\n",
    "    maml.resample_parameters(is_final=True)\n",
    "    maml_model = deepcopy(maml.model)\n",
    "    maml_train_loss_history, maml_test_loss_history = \\\n",
    "        eval_model(maml_model, permuted_animefaces_train_loader, permuted_animefaces_test_loader, \n",
    "                   batches_in_epoch=animefaces_batches_in_epoch, epochs=100, \n",
    "                   test_loss_interval=10*animefaces_batches_in_epoch, device=device)\n",
    "    \n",
    "    permuted_animefaces_base_runs_10.append(base_test_loss_history[1])\n",
    "    permuted_animefaces_base_runs_50.append(base_test_loss_history[5])\n",
    "    permuted_animefaces_base_runs_100.append(base_test_loss_history[10])\n",
    "    \n",
    "    permuted_animefaces_maml_runs_10.append(maml_test_loss_history[1])\n",
    "    permuted_animefaces_maml_runs_50.append(maml_test_loss_history[5])\n",
    "    permuted_animefaces_maml_runs_100.append(maml_test_loss_history[10])\n",
    "    \n",
    "    permuted_animefaces_deltaorthogonal_runs_10.append(deltaorthogonal_test_loss_history[1])\n",
    "    permuted_animefaces_deltaorthogonal_runs_50.append(deltaorthogonal_test_loss_history[5])\n",
    "    permuted_animefaces_deltaorthogonal_runs_100.append(deltaorthogonal_test_loss_history[10])\n",
    "    \n",
    "    permuted_animefaces_metainit_runs_10.append(metainit_test_loss_history[1])\n",
    "    permuted_animefaces_metainit_runs_50.append(metainit_test_loss_history[5])\n",
    "    permuted_animefaces_metainit_runs_100.append(metainit_test_loss_history[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline 10 epoch: \", np.mean(permuted_animefaces_base_runs_10), np.std(permuted_animefaces_base_runs_10, ddof=1))\n",
    "print(\"Baseline 50 epoch: \", np.mean(permuted_animefaces_base_runs_50), np.std(permuted_animefaces_base_runs_50, ddof=1))\n",
    "print(\"Baseline 100 epoch: \", np.mean(permuted_animefaces_base_runs_100), np.std(permuted_animefaces_base_runs_100, ddof=1))\n",
    "print()\n",
    "print(\"DIMAML 10 epoch: \", np.mean(permuted_animefaces_maml_runs_10), np.std(permuted_animefaces_maml_runs_10, ddof=1))\n",
    "print(\"DIMAML 50 epoch: \", np.mean(permuted_animefaces_maml_runs_50), np.std(permuted_animefaces_maml_runs_50, ddof=1))\n",
    "print(\"DIMAML 100 epoch: \", np.mean(permuted_animefaces_maml_runs_100), np.std(permuted_animefaces_maml_runs_100, ddof=1))\n",
    "print()\n",
    "print(\"MetaInit 10 epoch: \", np.mean(permuted_animefaces_metainit_runs_10), np.std(permuted_animefaces_metainit_runs_10, ddof=1))\n",
    "print(\"MetaInit 50 epoch: \", np.mean(permuted_animefaces_metainit_runs_50), np.std(permuted_animefaces_metainit_runs_50, ddof=1))\n",
    "print(\"MetaInit 100 epoch: \", np.mean(permuted_animefaces_metainit_runs_100), np.std(permuted_animefaces_metainit_runs_100, ddof=1))\n",
    "print()\n",
    "print(\"DeltaOrthogonal 10 epoch: \", np.mean(permuted_animefaces_deltaorthogonal_runs_10), np.std(permuted_animefaces_deltaorthogonal_runs_10, ddof=1))\n",
    "print(\"DeltaOrthogonal 50 epoch: \", np.mean(permuted_animefaces_deltaorthogonal_runs_50), np.std(permuted_animefaces_deltaorthogonal_runs_50, ddof=1))\n",
    "print(\"DeltaOrthogonal 100 epoch: \", np.mean(permuted_animefaces_deltaorthogonal_runs_100), np.std(permuted_animefaces_deltaorthogonal_runs_100, ddof=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
